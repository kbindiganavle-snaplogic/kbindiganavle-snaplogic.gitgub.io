


<!DOCTYPE html>
<html id="htmlId">
<head>
  <title>Coverage Report > ParquetWritingFormatter</title>
  <style type="text/css">
    @import "../../css/coverage.css";
    @import "../../css/highlight-idea.css";
  </style>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
</head>

<body>
<div class="content">
<div class="breadCrumbs">
Current scope:     <a href="../../index.html">all classes</a>
    <span class="separator">|</span>
    <a href="../index.html">com.snaplogic.snaps.hadoop</a>
</div>

<h1>Coverage Summary for Class: ParquetWritingFormatter (com.snaplogic.snaps.hadoop)</h1>

<table class="coverageStats">

<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">ParquetWritingFormatter</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/29)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/373)
  </span>
</td>
</tr>
  <tr>
    <td class="name">ParquetWritingFormatter$1</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/2)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/7)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">ParquetWritingFormatter$1$1</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">ParquetWritingFormatter$1$2</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">ParquetWritingFormatter$2</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">ParquetWritingFormatter$AbfsPartitionedFileObject</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/4)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">ParquetWritingFormatter$SupportedCodecs</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/2)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/10)
  </span>
</td>
  </tr>
<tr>
  <td class="name"><strong>Total</strong></td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/37)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/397)
  </span>
</td>
</tr>
</table>

<br/>
<br/>


<pre>
<div class="sourceCode" id="sourceCode"><i class="no-highlight">1</i>&nbsp;/*
<i class="no-highlight">2</i>&nbsp; * SnapLogic - Data Integration
<i class="no-highlight">3</i>&nbsp; *
<i class="no-highlight">4</i>&nbsp; * Copyright (C) 2015, SnapLogic, Inc.  All rights reserved.
<i class="no-highlight">5</i>&nbsp; *
<i class="no-highlight">6</i>&nbsp; * This program is licensed under the terms of
<i class="no-highlight">7</i>&nbsp; * the SnapLogic Commercial Subscription agreement.
<i class="no-highlight">8</i>&nbsp; *
<i class="no-highlight">9</i>&nbsp; * &quot;SnapLogic&quot; is a trademark of SnapLogic, Inc.
<i class="no-highlight">10</i>&nbsp; */
<i class="no-highlight">11</i>&nbsp;package com.snaplogic.snaps.hadoop;
<i class="no-highlight">12</i>&nbsp;
<i class="no-highlight">13</i>&nbsp;import com.amazonaws.AmazonClientException;
<i class="no-highlight">14</i>&nbsp;import com.amazonaws.util.Throwables;
<i class="no-highlight">15</i>&nbsp;import com.google.common.collect.ImmutableSet;
<i class="no-highlight">16</i>&nbsp;import com.google.common.collect.Lists;
<i class="no-highlight">17</i>&nbsp;import com.google.inject.AbstractModule;
<i class="no-highlight">18</i>&nbsp;import com.google.inject.Inject;
<i class="no-highlight">19</i>&nbsp;import com.google.inject.Key;
<i class="no-highlight">20</i>&nbsp;import com.google.inject.Module;
<i class="no-highlight">21</i>&nbsp;import com.google.inject.Scopes;
<i class="no-highlight">22</i>&nbsp;import com.google.inject.TypeLiteral;
<i class="no-highlight">23</i>&nbsp;import com.snaplogic.account.api.Account;
<i class="no-highlight">24</i>&nbsp;import com.snaplogic.account.api.capabilities.Accounts;
<i class="no-highlight">25</i>&nbsp;import com.snaplogic.account.api.capabilities.MultiAccountBinding;
<i class="no-highlight">26</i>&nbsp;import com.snaplogic.api.ConfigurationException;
<i class="no-highlight">27</i>&nbsp;import com.snaplogic.api.ExecutionException;
<i class="no-highlight">28</i>&nbsp;import com.snaplogic.api.InputSchemaProvider;
<i class="no-highlight">29</i>&nbsp;import com.snaplogic.api.LifecycleEvent;
<i class="no-highlight">30</i>&nbsp;import com.snaplogic.api.SuggestExecutionProvider;
<i class="no-highlight">31</i>&nbsp;import com.snaplogic.api.ViewProvider;
<i class="no-highlight">32</i>&nbsp;import com.snaplogic.common.SnapType;
<i class="no-highlight">33</i>&nbsp;import com.snaplogic.common.properties.SnapProperty;
<i class="no-highlight">34</i>&nbsp;import com.snaplogic.common.properties.builders.PropertyBuilder;
<i class="no-highlight">35</i>&nbsp;import com.snaplogic.common.properties.builders.ViewBuilder;
<i class="no-highlight">36</i>&nbsp;import com.snaplogic.snap.api.Document;
<i class="no-highlight">37</i>&nbsp;import com.snaplogic.snap.api.PropertyValues;
<i class="no-highlight">38</i>&nbsp;import com.snaplogic.snap.api.SnapCategory;
<i class="no-highlight">39</i>&nbsp;import com.snaplogic.snap.api.SnapDataException;
<i class="no-highlight">40</i>&nbsp;import com.snaplogic.snap.api.SuggestViewAbortException;
<i class="no-highlight">41</i>&nbsp;import com.snaplogic.snap.api.ViewCategory;
<i class="no-highlight">42</i>&nbsp;import com.snaplogic.snap.api.authentication.KerberosAuthentication;
<i class="no-highlight">43</i>&nbsp;import com.snaplogic.snap.api.authentication.KerberosConfig;
<i class="no-highlight">44</i>&nbsp;import com.snaplogic.snap.api.binary.AbfsAccount;
<i class="no-highlight">45</i>&nbsp;import com.snaplogic.snap.api.binary.BinaryDefaultAccount;
<i class="no-highlight">46</i>&nbsp;import com.snaplogic.snap.api.binary.BinaryUtils;
<i class="no-highlight">47</i>&nbsp;import com.snaplogic.snap.api.binary.ParquetWriterBuilder;
<i class="no-highlight">48</i>&nbsp;import com.snaplogic.snap.api.capabilities.Category;
<i class="no-highlight">49</i>&nbsp;import com.snaplogic.snap.api.capabilities.Errors;
<i class="no-highlight">50</i>&nbsp;import com.snaplogic.snap.api.capabilities.General;
<i class="no-highlight">51</i>&nbsp;import com.snaplogic.snap.api.capabilities.Inputs;
<i class="no-highlight">52</i>&nbsp;import com.snaplogic.snap.api.capabilities.Version;
<i class="no-highlight">53</i>&nbsp;import com.snaplogic.snap.api.capabilities.ViewType;
<i class="no-highlight">54</i>&nbsp;import com.snaplogic.snap.api.editor.EditorPropertyFactory;
<i class="no-highlight">55</i>&nbsp;import com.snaplogic.snap.schema.api.ObjectSchema;
<i class="no-highlight">56</i>&nbsp;import com.snaplogic.snap.schema.api.Schema;
<i class="no-highlight">57</i>&nbsp;import com.snaplogic.snap.schema.api.SchemaBuilder;
<i class="no-highlight">58</i>&nbsp;import com.snaplogic.snap.schema.api.SchemaProvider;
<i class="no-highlight">59</i>&nbsp;import com.snaplogic.snap.view.InputView;
<i class="no-highlight">60</i>&nbsp;import com.snaplogic.snaps.hadoop.common.HdfsUtils;
<i class="no-highlight">61</i>&nbsp;import com.snaplogic.snaps.hadoop.parquet.SchemaConverter;
<i class="no-highlight">62</i>&nbsp;import com.snaplogic.snaps.hadoop.sastoken.SasTokenUtils;
<i class="no-highlight">63</i>&nbsp;import com.snaplogic.snaps.hadoop.util.HadoopConfigUtils;
<i class="no-highlight">64</i>&nbsp;import com.snaplogic.snaps.hadoop.util.S3ErrorHandler;
<i class="no-highlight">65</i>&nbsp;import com.snaplogic.util.SchemaFactory;
<i class="no-highlight">66</i>&nbsp;import com.snaplogic.util.SnapObjectMapper;
<i class="no-highlight">67</i>&nbsp;import com.snaplogic.util.ViewNameUtil;
<i class="no-highlight">68</i>&nbsp;
<i class="no-highlight">69</i>&nbsp;import org.apache.commons.collections.CollectionUtils;
<i class="no-highlight">70</i>&nbsp;import org.apache.commons.collections.MapUtils;
<i class="no-highlight">71</i>&nbsp;import org.apache.commons.io.IOUtils;
<i class="no-highlight">72</i>&nbsp;import org.apache.commons.lang3.StringUtils;
<i class="no-highlight">73</i>&nbsp;import org.apache.hadoop.conf.Configuration;
<i class="no-highlight">74</i>&nbsp;import org.apache.hadoop.fs.Path;
<i class="no-highlight">75</i>&nbsp;import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
<i class="no-highlight">76</i>&nbsp;import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
<i class="no-highlight">77</i>&nbsp;import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
<i class="no-highlight">78</i>&nbsp;import org.apache.parquet.hadoop.BadConfigurationException;
<i class="no-highlight">79</i>&nbsp;import org.apache.parquet.hadoop.ParquetFileWriter;
<i class="no-highlight">80</i>&nbsp;import org.apache.parquet.hadoop.ParquetWriter;
<i class="no-highlight">81</i>&nbsp;import org.apache.parquet.hadoop.metadata.CompressionCodecName;
<i class="no-highlight">82</i>&nbsp;import org.apache.parquet.schema.MessageType;
<i class="no-highlight">83</i>&nbsp;import org.apache.parquet.schema.MessageTypeParser;
<i class="no-highlight">84</i>&nbsp;import org.slf4j.Logger;
<i class="no-highlight">85</i>&nbsp;import org.slf4j.LoggerFactory;
<i class="no-highlight">86</i>&nbsp;
<i class="no-highlight">87</i>&nbsp;import java.io.File;
<i class="no-highlight">88</i>&nbsp;import java.io.IOException;
<i class="no-highlight">89</i>&nbsp;import java.math.BigInteger;
<i class="no-highlight">90</i>&nbsp;import java.security.PrivilegedAction;
<i class="no-highlight">91</i>&nbsp;import java.util.ArrayList;
<i class="no-highlight">92</i>&nbsp;import java.util.Collection;
<i class="no-highlight">93</i>&nbsp;import java.util.EnumSet;
<i class="no-highlight">94</i>&nbsp;import java.util.LinkedHashMap;
<i class="no-highlight">95</i>&nbsp;import java.util.List;
<i class="no-highlight">96</i>&nbsp;import java.util.Map;
<i class="no-highlight">97</i>&nbsp;import java.util.Objects;
<i class="no-highlight">98</i>&nbsp;import java.util.Set;
<i class="no-highlight">99</i>&nbsp;import java.util.regex.Pattern;
<i class="no-highlight">100</i>&nbsp;
<i class="no-highlight">101</i>&nbsp;import static com.snaplogic.snap.api.binary.BinaryUtils.DIRECTORY_PROP;
<i class="no-highlight">102</i>&nbsp;import static com.snaplogic.snap.api.binary.BinaryUtils.FILE_PROP;
<i class="no-highlight">103</i>&nbsp;import static com.snaplogic.snap.api.binary.BinaryUtils.URL_SEPARATOR;
<i class="no-highlight">104</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.CEILING;
<i class="no-highlight">105</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.DOWN;
<i class="no-highlight">106</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.FLOOR;
<i class="no-highlight">107</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.HALF_DOWN;
<i class="no-highlight">108</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.HALF_EVEN;
<i class="no-highlight">109</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.HALF_UP;
<i class="no-highlight">110</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.TRUNCATE;
<i class="no-highlight">111</i>&nbsp;import static com.snaplogic.snap.api.binary.Constants.UP;
<i class="no-highlight">112</i>&nbsp;import static com.snaplogic.snap.api.fs.JfsUtils.PROTOCOL_ABFS;
<i class="no-highlight">113</i>&nbsp;import static com.snaplogic.snap.api.fs.hadoop.Constants.DEFAULT_DIRECTORY;
<i class="no-highlight">114</i>&nbsp;import static com.snaplogic.snaps.hadoop.Constants.DOLLAR;
<i class="no-highlight">115</i>&nbsp;import static com.snaplogic.snaps.hadoop.Messages.*;
<i class="no-highlight">116</i>&nbsp;import static com.snaplogic.snaps.hadoop.common.HdfsUtils.configToMap;
<i class="no-highlight">117</i>&nbsp;import static com.snaplogic.snaps.hadoop.sastoken.SasTokenUtils.NewHadoopPathAndConfig;
<i class="no-highlight">118</i>&nbsp;
<b class="nc"><i class="no-highlight">119</i>&nbsp;/**</b>
<i class="no-highlight">120</i>&nbsp; * Snap that accepts documents and converts them to parquet columnar format.
<i class="no-highlight">121</i>&nbsp; */
<b class="nc"><i class="no-highlight">122</i>&nbsp;@General(title = PARQUET_FORMATTER_LABEL, purpose = PARQUET_FORMATTER_DESC)</b>
<b class="nc"><i class="no-highlight">123</i>&nbsp;@Version(snap = 1)</b>
<b class="nc"><i class="no-highlight">124</i>&nbsp;@Inputs(min = 1, max = 2, accepts = {ViewType.DOCUMENT})</b>
<i class="no-highlight">125</i>&nbsp;@Errors(min = 1, max = 1, offers = {ViewType.DOCUMENT})
<i class="no-highlight">126</i>&nbsp;@Category(snap = SnapCategory.WRITE)
<i class="no-highlight">127</i>&nbsp;@Accounts(provides = {HadoopS3Account.class, HadoopS3DynamicAccount.class, KerberosAccount.class,
<i class="no-highlight">128</i>&nbsp;        BinaryAzureAccount.class, AzureDataLakeAccount.class, AbfsAccount.class}, optional = true)
<i class="no-highlight">129</i>&nbsp;public class ParquetWritingFormatter extends AbstractHdfsWriter
<i class="no-highlight">130</i>&nbsp;        implements MultiAccountBinding&lt;Account&lt;String&gt;&gt;, InputSchemaProvider , ViewProvider,
<i class="no-highlight">131</i>&nbsp;        SuggestExecutionProvider {
<i class="no-highlight">132</i>&nbsp;    private static final Logger LOG = LoggerFactory.getLogger(ParquetWritingFormatter.class);
<i class="no-highlight">133</i>&nbsp;    private static final Pattern COMMENT_LINE = Pattern.compile(&quot;^(\\s)*#.*&quot;);
<i class="no-highlight">134</i>&nbsp;    private static final boolean USER_IMPERSONATION_DEFAULT = Boolean.FALSE;
<i class="no-highlight">135</i>&nbsp;    private static final String COMPRESSION = &quot;com.snaplogic.snaps.hadoop.parquet.compression&quot;;
<i class="no-highlight">136</i>&nbsp;    private static final String PARTITION_KEY_PROP = &quot;partitionKey&quot;;
<i class="no-highlight">137</i>&nbsp;    private static final String PARTITION_BY_PROP = &quot;partitionBy&quot;;
<i class="no-highlight">138</i>&nbsp;    private static final String VALUE = &quot;value&quot;;
<i class="no-highlight">139</i>&nbsp;    private static final String COMMENT_PREFIX = &quot;#&quot;;
<i class="no-highlight">140</i>&nbsp;    public static final String USE_METASTORE = &quot;useMetastore&quot;;
<i class="no-highlight">141</i>&nbsp;    private static final String DATA_VIEW_NAME = &quot;input0&quot;;
<i class="no-highlight">142</i>&nbsp;    private static final String COL_NAME_KEY = &quot;col_name&quot;;
<i class="no-highlight">143</i>&nbsp;    private static final String DATA_TYPE_KEY = &quot;data_type&quot;;
<i class="no-highlight">144</i>&nbsp;    protected static final String SCHEMA = &quot;schema&quot;;
<i class="no-highlight">145</i>&nbsp;    protected static final String PARTITION_KEY = &quot;partitionkey&quot;;
<i class="no-highlight">146</i>&nbsp;    protected static final String PARTITION_VALUE = &quot;partitionvalue&quot;;
<i class="no-highlight">147</i>&nbsp;    private static final String ROUND_DECIMAL_PROP = &quot;roundingMode&quot;;
<i class="no-highlight">148</i>&nbsp;    private static final Set&lt;String&gt; ROUNDING_MODE_TYPES = ImmutableSet.of(
<b class="nc"><i class="no-highlight">149</i>&nbsp;            HALF_UP, HALF_DOWN, HALF_EVEN, UP, DOWN, CEILING, FLOOR, TRUNCATE);</b>
<i class="no-highlight">150</i>&nbsp;
<i class="no-highlight">151</i>&nbsp;    // instance variables
<i class="no-highlight">152</i>&nbsp;    @Inject
<i class="no-highlight">153</i>&nbsp;    protected EditorPropertyFactory editorPropertyFactory;
<i class="no-highlight">154</i>&nbsp;    @Inject
<i class="no-highlight">155</i>&nbsp;    private BinaryUtils binaryUtils;
<i class="no-highlight">156</i>&nbsp;    @Inject
<i class="no-highlight">157</i>&nbsp;    private SchemaFactory schemaFactory;
<i class="no-highlight">158</i>&nbsp;    @Inject
<i class="no-highlight">159</i>&nbsp;    private ViewNameUtil viewNameUtil;
<i class="no-highlight">160</i>&nbsp;    @Inject
<i class="no-highlight">161</i>&nbsp;    private SnapObjectMapper snapObjectMapper;
<i class="no-highlight">162</i>&nbsp;    private List&lt;List&lt;Object&gt;&gt; partitionValues = new ArrayList&lt;&gt;();
<b class="nc"><i class="no-highlight">163</i>&nbsp;    protected MessageType schema;</b>
<i class="no-highlight">164</i>&nbsp;    CompressionCodecName codec;
<i class="no-highlight">165</i>&nbsp;    private List&lt;String&gt; partitionBy;
<b class="nc"><i class="no-highlight">166</i>&nbsp;    private Map&lt;String, Object&gt; parquetSchema;</b>
<b class="nc"><i class="no-highlight">167</i>&nbsp;    private Map&lt;String, ParquetWriter&gt; parquetWriters;</b>
<i class="no-highlight">168</i>&nbsp;    private Map&lt;String, AbfsPartitionedFileObject&gt; abfsPartitionedFilePathMap;
<b class="nc"><i class="no-highlight">169</i>&nbsp;    private String partitionDirPath;</b>
<b class="nc"><i class="no-highlight">170</i>&nbsp;    private String schemaViewName;</b>
<i class="no-highlight">171</i>&nbsp;    private boolean isExecuteDuringPreview;
<b class="nc"><i class="no-highlight">172</i>&nbsp;    private boolean fileCreated;</b>
<b class="nc"><i class="no-highlight">173</i>&nbsp;    private String roundingMode;</b>
<i class="no-highlight">174</i>&nbsp;
<i class="no-highlight">175</i>&nbsp;    @Override
<i class="no-highlight">176</i>&nbsp;    public Module getManagedAccountModule(final Account&lt;String&gt; account) {
<i class="no-highlight">177</i>&nbsp;        return new AbstractModule() {
<i class="no-highlight">178</i>&nbsp;            @Override
<i class="no-highlight">179</i>&nbsp;            protected void configure() {
<i class="no-highlight">180</i>&nbsp;                if (account == null) {
<b class="nc"><i class="no-highlight">181</i>&nbsp;                    bind(Key.get(new TypeLiteral&lt;Account&lt;String&gt;&gt;() {</b>
<b class="nc"><i class="no-highlight">182</i>&nbsp;                    }))</b>
<i class="no-highlight">183</i>&nbsp;                            .to(BinaryDefaultAccount.class)
<i class="no-highlight">184</i>&nbsp;                            .in(Scopes.SINGLETON);
<b class="nc"><i class="no-highlight">185</i>&nbsp;                } else {</b>
<i class="no-highlight">186</i>&nbsp;                    bind(Key.get(new TypeLiteral&lt;Account&lt;String&gt;&gt;() {
<i class="no-highlight">187</i>&nbsp;                    })).toInstance(account);
<i class="no-highlight">188</i>&nbsp;                }
<i class="no-highlight">189</i>&nbsp;            }
<b class="nc"><i class="no-highlight">190</i>&nbsp;        };</b>
<i class="no-highlight">191</i>&nbsp;    }
<b class="nc"><i class="no-highlight">192</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">193</i>&nbsp;    @Override</b>
<b class="nc"><i class="no-highlight">194</i>&nbsp;    public void defineProperties(PropertyBuilder propertyBuilder) {</b>
<i class="no-highlight">195</i>&nbsp;        if (account instanceof HadoopS3Account || account instanceof HadoopS3DynamicAccount) {
<b class="nc"><i class="no-highlight">196</i>&nbsp;            binaryUtils.defineFileProperties(propertyBuilder, DEFAULT_DIRECTORY, account, false,</b>
<b class="nc"><i class="no-highlight">197</i>&nbsp;                    abfsRestUtility);</b>
<b class="nc"><i class="no-highlight">198</i>&nbsp;        } else {</b>
<b class="nc"><i class="no-highlight">199</i>&nbsp;            binaryUtils.definePropertiesWithSuggestions(propertyBuilder, DEFAULT_DIRECTORY, false,</b>
<b class="nc"><i class="no-highlight">200</i>&nbsp;                    new BinaryUtils.HdfsFileSuggester(DIRECTORY_PROP, new WriteHdfsSupplier(),</b>
<b class="nc"><i class="no-highlight">201</i>&nbsp;                            abfsRestUtility),</b>
<i class="no-highlight">202</i>&nbsp;                    new BinaryUtils.HdfsFileSuggester(FILE_PROP, new WriteHdfsSupplier(),
<b class="nc"><i class="no-highlight">203</i>&nbsp;                            abfsRestUtility));</b>
<i class="no-highlight">204</i>&nbsp;            propertyBuilder.describe(HdfsUtils.USER_IMPERSONATION_ENABLED,
<b class="nc"><i class="no-highlight">205</i>&nbsp;                    USER_IMPERSONATION_TITLE, USER_IMPERSONATION_DESC)</b>
<i class="no-highlight">206</i>&nbsp;                    .type(SnapType.BOOLEAN)
<b class="nc"><i class="no-highlight">207</i>&nbsp;                    .defaultValue(USER_IMPERSONATION_DEFAULT)</b>
<b class="nc"><i class="no-highlight">208</i>&nbsp;                    .add();</b>
<b class="nc"><i class="no-highlight">209</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">210</i>&nbsp;        RCSnapUtil.addMetastoreWritePropsTo(propertyBuilder);</b>
<b class="nc"><i class="no-highlight">211</i>&nbsp;        propertyBuilder.describe(USE_METASTORE, USE_METASTORE_LABEL, USE_METASTORE_DESC)</b>
<i class="no-highlight">212</i>&nbsp;                .optional()
<b class="nc"><i class="no-highlight">213</i>&nbsp;                .defaultValue(Boolean.FALSE)</b>
<i class="no-highlight">214</i>&nbsp;                .type(SnapType.BOOLEAN)
<b class="nc"><i class="no-highlight">215</i>&nbsp;                .add();</b>
<b class="nc"><i class="no-highlight">216</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">217</i>&nbsp;        editorPropertyFactory.createEditorProperty(propertyBuilder, new MessageTypeSuggester(),</b>
<b class="nc"><i class="no-highlight">218</i>&nbsp;                SnapProperty.EditorType.TEXT, PARQUET_PROP_EDIT_SCHEMA, PARQUET_DESC_EDIT_SCHEMA)</b>
<b class="nc"><i class="no-highlight">219</i>&nbsp;                .add();</b>
<b class="nc"><i class="no-highlight">220</i>&nbsp;</b>
<i class="no-highlight">221</i>&nbsp;        propertyBuilder.describe(PARQUET_PROP_COMPRESSION, PARQUET_DESC_COMPRESSION)
<b class="nc"><i class="no-highlight">222</i>&nbsp;                .withAllowedValues(EnumSet.allOf(SupportedCodecs.class))</b>
<b class="nc"><i class="no-highlight">223</i>&nbsp;                .defaultValue(SupportedCodecs.NONE)</b>
<i class="no-highlight">224</i>&nbsp;                .required()
<i class="no-highlight">225</i>&nbsp;                .add();
<i class="no-highlight">226</i>&nbsp;
<i class="no-highlight">227</i>&nbsp;        SnapProperty entry = propertyBuilder.describe(PARTITION_KEY_PROP, PARTITION_BY_LABEL,
<i class="no-highlight">228</i>&nbsp;                PARTITION_BY_DESC)
<b class="nc"><i class="no-highlight">229</i>&nbsp;                .schemaAware(SnapProperty.DecoratorType.ACCEPTS_SCHEMA)</b>
<i class="no-highlight">230</i>&nbsp;                .build();
<i class="no-highlight">231</i>&nbsp;        propertyBuilder.describe(PARTITION_BY_PROP, PARTITION_BY_LABEL, PARTITION_BY_DESC)
<i class="no-highlight">232</i>&nbsp;                .type(SnapType.TABLE)
<i class="no-highlight">233</i>&nbsp;                .withEntry(entry)
<b class="nc"><i class="no-highlight">234</i>&nbsp;                .add();</b>
<b class="nc"><i class="no-highlight">235</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">236</i>&nbsp;        if (supportAzureSasUri()) {</b>
<b class="nc"><i class="no-highlight">237</i>&nbsp;            super.addSasUriProperties(propertyBuilder);</b>
<b class="nc"><i class="no-highlight">238</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">239</i>&nbsp;        propertyBuilder.describe(ROUND_DECIMAL_PROP, ROUND_UP_DECIMAL_VALUES_LABEL, ROUND_UP_DECIMAL_VALUES_DESC)</b>
<b class="nc"><i class="no-highlight">240</i>&nbsp;                .optional()</b>
<b class="nc"><i class="no-highlight">241</i>&nbsp;                .withAllowedValues(ROUNDING_MODE_TYPES)</b>
<b class="nc"><i class="no-highlight">242</i>&nbsp;                .defaultValue(HALF_UP)</b>
<b class="nc"><i class="no-highlight">243</i>&nbsp;                .add();</b>
<i class="no-highlight">244</i>&nbsp;    }
<b class="nc"><i class="no-highlight">245</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">246</i>&nbsp;    @Override</b>
<b class="nc"><i class="no-highlight">247</i>&nbsp;    protected boolean supportAzureSasUri() {</b>
<b class="nc"><i class="no-highlight">248</i>&nbsp;        return true;</b>
<b class="nc"><i class="no-highlight">249</i>&nbsp;    }</b>
<i class="no-highlight">250</i>&nbsp;
<b class="nc"><i class="no-highlight">251</i>&nbsp;    @Override</b>
<b class="nc"><i class="no-highlight">252</i>&nbsp;    public void defineInputSchema(SchemaProvider provider) {</b>
<b class="nc"><i class="no-highlight">253</i>&nbsp;        Collection&lt;String&gt; inputViewNames = provider.getRegisteredInputViewNames();</b>
<i class="no-highlight">254</i>&nbsp;        List&lt;String&gt; sortedViewNames = viewNameUtil.sortViewNames(inputViewNames, DATA_VIEW_NAME);
<i class="no-highlight">255</i>&nbsp;        if (sortedViewNames.size() &gt; 1) {
<i class="no-highlight">256</i>&nbsp;            schemaViewName = sortedViewNames.get(1);
<i class="no-highlight">257</i>&nbsp;            SchemaBuilder builder = provider.getSchemaBuilder(schemaViewName);
<i class="no-highlight">258</i>&nbsp;            Schema colNameSchema = provider.createSchema(SnapType.STRING, COL_NAME_KEY);
<b class="nc"><i class="no-highlight">259</i>&nbsp;            Schema dataTypeSchema = provider.createSchema(SnapType.STRING, DATA_TYPE_KEY);</b>
<b class="nc"><i class="no-highlight">260</i>&nbsp;            builder.withChildSchema(colNameSchema)</b>
<b class="nc"><i class="no-highlight">261</i>&nbsp;                    .withChildSchema(dataTypeSchema)</b>
<i class="no-highlight">262</i>&nbsp;                    .build();
<b class="nc"><i class="no-highlight">263</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">264</i>&nbsp;        if (schema != null &amp;&amp; schemaFactory != null) {</b>
<b class="nc"><i class="no-highlight">265</i>&nbsp;            SchemaConverter schemaConverter = new SchemaConverter(schemaFactory);</b>
<i class="no-highlight">266</i>&nbsp;            ObjectSchema objectSchema = schemaConverter.convert(schema);
<b class="nc"><i class="no-highlight">267</i>&nbsp;            if (!inputViewNames.isEmpty()) {</b>
<b class="nc"><i class="no-highlight">268</i>&nbsp;                provider.setProvidedSchemaForViewName(DATA_VIEW_NAME, objectSchema);</b>
<b class="nc"><i class="no-highlight">269</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">270</i>&nbsp;        } else {</b>
<i class="no-highlight">271</i>&nbsp;            LOG.debug(String.format(
<i class="no-highlight">272</i>&nbsp;                    CANNOT_DEFINE_SCHEMA, schema == null, schemaFactory == null));
<b class="nc"><i class="no-highlight">273</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">274</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">275</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">276</i>&nbsp;    @Override</b>
<b class="nc"><i class="no-highlight">277</i>&nbsp;    public void configure(PropertyValues propertyValues) throws ConfigurationException {</b>
<b class="nc"><i class="no-highlight">278</i>&nbsp;        super.configure(propertyValues);</b>
<b class="nc"><i class="no-highlight">279</i>&nbsp;        initializeTempDir(propertyValues);</b>
<b class="nc"><i class="no-highlight">280</i>&nbsp;        fileAction = FileAction.OVERWRITE;</b>
<b class="nc"><i class="no-highlight">281</i>&nbsp;</b>
<i class="no-highlight">282</i>&nbsp;        List&lt;String&gt; viewNames = viewNameUtil.sortViewNames(inputViews.names(), DATA_VIEW_NAME);
<b class="nc"><i class="no-highlight">283</i>&nbsp;        if (viewNames.size() &gt; 1) {</b>
<b class="nc"><i class="no-highlight">284</i>&nbsp;            schemaViewName = viewNames.get(1);</b>
<b class="nc"><i class="no-highlight">285</i>&nbsp;        }</b>
<i class="no-highlight">286</i>&nbsp;        setSchema(propertyValues);
<i class="no-highlight">287</i>&nbsp;        String compression = propertyValues.get(PARQUET_PROP_COMPRESSION);
<b class="nc"><i class="no-highlight">288</i>&nbsp;        codec = SupportedCodecs.toCompressionCodecName(compression);</b>
<b class="nc"><i class="no-highlight">289</i>&nbsp;        isExecuteDuringPreview = propertyValues.get(</b>
<b class="nc"><i class="no-highlight">290</i>&nbsp;                com.snaplogic.common.Constants.EXECUTABLE_DURING_SUGGEST);</b>
<b class="nc"><i class="no-highlight">291</i>&nbsp;        // init partition-by property</b>
<i class="no-highlight">292</i>&nbsp;        List&lt;Map&lt;String, Object&gt;&gt; partitionByProps = propertyValues.get(PARTITION_BY_PROP);
<i class="no-highlight">293</i>&nbsp;        if (CollectionUtils.isNotEmpty(partitionByProps)) {
<b class="nc"><i class="no-highlight">294</i>&nbsp;            partitionBy = new ArrayList&lt;&gt;(partitionByProps.size());</b>
<i class="no-highlight">295</i>&nbsp;            for (Map&lt;String, Object&gt; partitionByProp : partitionByProps) {
<i class="no-highlight">296</i>&nbsp;                String key = ((Map&lt;String, String&gt;) partitionByProp.get(PARTITION_KEY_PROP))
<i class="no-highlight">297</i>&nbsp;                        .get(VALUE);
<b class="nc"><i class="no-highlight">298</i>&nbsp;                if (StringUtils.isNotBlank(key)) {</b>
<i class="no-highlight">299</i>&nbsp;                    if (key.startsWith(DOLLAR)) {
<b class="nc"><i class="no-highlight">300</i>&nbsp;                        key = key.substring(1);</b>
<i class="no-highlight">301</i>&nbsp;                    }
<i class="no-highlight">302</i>&nbsp;                    if (StringUtils.isNotEmpty(key)) {
<b class="nc"><i class="no-highlight">303</i>&nbsp;                        partitionBy.add(key);</b>
<b class="nc"><i class="no-highlight">304</i>&nbsp;                        continue;</b>
<b class="nc"><i class="no-highlight">305</i>&nbsp;                    }</b>
<i class="no-highlight">306</i>&nbsp;                }
<i class="no-highlight">307</i>&nbsp;                throw new ConfigurationException(ERR_EMPTY_PROPERTY)
<b class="nc"><i class="no-highlight">308</i>&nbsp;                        .formatWith(PARTITION_BY_LABEL)</b>
<b class="nc"><i class="no-highlight">309</i>&nbsp;                        .withReason(REASON_EMPTY_PROPERTY)</b>
<i class="no-highlight">310</i>&nbsp;                        .withResolution(RESOLUTION_ADDRESS_ISSUE);
<b class="nc"><i class="no-highlight">311</i>&nbsp;            }</b>
<i class="no-highlight">312</i>&nbsp;        }
<i class="no-highlight">313</i>&nbsp;        handleSasUri(propertyValues);
<i class="no-highlight">314</i>&nbsp;        roundingMode = propertyValues.get(ROUND_DECIMAL_PROP);
<b class="nc"><i class="no-highlight">315</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">316</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">317</i>&nbsp;    private void setSchema(PropertyValues propertyValues) {</b>
<b class="nc"><i class="no-highlight">318</i>&nbsp;        if (schemaViewName != null) {</b>
<b class="nc"><i class="no-highlight">319</i>&nbsp;            //If the schema view is opened, we ignore other ways of getting schema</b>
<i class="no-highlight">320</i>&nbsp;            schema = null;
<i class="no-highlight">321</i>&nbsp;            return;
<i class="no-highlight">322</i>&nbsp;        }
<b class="nc"><i class="no-highlight">323</i>&nbsp;        Boolean useMetastore = propertyValues.get(USE_METASTORE);</b>
<b class="nc"><i class="no-highlight">324</i>&nbsp;        if (useMetastore == null) {</b>
<i class="no-highlight">325</i>&nbsp;            useMetastore = false;
<i class="no-highlight">326</i>&nbsp;        }
<i class="no-highlight">327</i>&nbsp;        String schemaToUse;
<b class="nc"><i class="no-highlight">328</i>&nbsp;        if (useMetastore) {</b>
<b class="nc"><i class="no-highlight">329</i>&nbsp;            schemaToUse = MessageTypeSuggester.getHiveSuggestion(propertyValues);</b>
<b class="nc"><i class="no-highlight">330</i>&nbsp;        } else {</b>
<b class="nc"><i class="no-highlight">331</i>&nbsp;            schemaToUse = getSchemaFromEditor(propertyValues);</b>
<i class="no-highlight">332</i>&nbsp;        }
<b class="nc"><i class="no-highlight">333</i>&nbsp;</b>
<i class="no-highlight">334</i>&nbsp;        try {
<i class="no-highlight">335</i>&nbsp;            schema = MessageTypeParser.parseMessageType(schemaToUse);
<i class="no-highlight">336</i>&nbsp;        } catch (IllegalArgumentException e) {
<i class="no-highlight">337</i>&nbsp;            throw new ConfigurationException(e, PARQUET_INVALID_SCHEMA)
<i class="no-highlight">338</i>&nbsp;                    .withReason(PARQUET_UNABLE_TO_PARSE);
<i class="no-highlight">339</i>&nbsp;        }
<i class="no-highlight">340</i>&nbsp;    }
<b class="nc"><i class="no-highlight">341</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">342</i>&nbsp;    private String getSchemaFromEditor(PropertyValues propertyValues) {</b>
<i class="no-highlight">343</i>&nbsp;        String messageString = editorPropertyFactory.getEditorContent(propertyValues);
<i class="no-highlight">344</i>&nbsp;        return cleanUpSchema(messageString);
<b class="nc"><i class="no-highlight">345</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">346</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">347</i>&nbsp;    private String cleanUpSchema(String schemaString) {</b>
<b class="nc"><i class="no-highlight">348</i>&nbsp;        schemaString = schemaString != null ? schemaString : &quot;&quot;;</b>
<i class="no-highlight">349</i>&nbsp;        StringBuilder builder = new StringBuilder();
<i class="no-highlight">350</i>&nbsp;        for (String line : schemaString.split(&quot;\n&quot;)) {
<b class="nc"><i class="no-highlight">351</i>&nbsp;            builder.append(COMMENT_LINE.matcher(line).replaceAll(&quot;&quot;));</b>
<b class="nc"><i class="no-highlight">352</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">353</i>&nbsp;        return builder.toString();</b>
<i class="no-highlight">354</i>&nbsp;    }
<i class="no-highlight">355</i>&nbsp;
<i class="no-highlight">356</i>&nbsp;    /**
<b class="nc"><i class="no-highlight">357</i>&nbsp;     * Write to a single, unpartitioned output file. Only create a writer if there&#39;s at</b>
<b class="nc"><i class="no-highlight">358</i>&nbsp;     * least one record.</b>
<b class="nc"><i class="no-highlight">359</i>&nbsp;     */</b>
<i class="no-highlight">360</i>&nbsp;    protected void doWorkUnpartitioned() {
<b class="nc"><i class="no-highlight">361</i>&nbsp;        ParquetWriter writer = null;</b>
<b class="nc"><i class="no-highlight">362</i>&nbsp;        Document lastDocument = null;</b>
<i class="no-highlight">363</i>&nbsp;
<i class="no-highlight">364</i>&nbsp;        try {
<b class="nc"><i class="no-highlight">365</i>&nbsp;            for (Document document : inputViews.documents(inputViews.get(DATA_VIEW_NAME))) {</b>
<b class="nc"><i class="no-highlight">366</i>&nbsp;                lastDocument = document;</b>
<b class="nc"><i class="no-highlight">367</i>&nbsp;                if (azureSasUriProperties != null) {</b>
<b class="nc"><i class="no-highlight">368</i>&nbsp;                    extractSasToken(document);</b>
<b class="nc"><i class="no-highlight">369</i>&nbsp;                }</b>
<b class="nc"><i class="no-highlight">370</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">371</i>&nbsp;                Map&lt;String, Object&gt; dataMap = document.get(Map.class);</b>
<b class="nc"><i class="no-highlight">372</i>&nbsp;                if (writer == null) {</b>
<b class="nc"><i class="no-highlight">373</i>&nbsp;                    if (StringUtils.startsWithIgnoreCase(filenameNoAuth, PROTOCOL_ABFS)) {</b>
<b class="nc"><i class="no-highlight">374</i>&nbsp;                        /* If ABFS, create a temp file to write the parquet data and send the</b>
<b class="nc"><i class="no-highlight">375</i>&nbsp;                        file as input stream to ABFS REST api.</b>
<b class="nc"><i class="no-highlight">376</i>&nbsp;                         */</b>
<b class="nc"><i class="no-highlight">377</i>&nbsp;                        setTempFile();</b>
<b class="nc"><i class="no-highlight">378</i>&nbsp;                        writer = createParquetWriter(filename);</b>
<b class="nc"><i class="no-highlight">379</i>&nbsp;                        fileCreated = true;</b>
<b class="nc"><i class="no-highlight">380</i>&nbsp;                    } else {</b>
<b class="nc"><i class="no-highlight">381</i>&nbsp;                        writer = createParquetWriter(filenameNoAuth);</b>
<b class="nc"><i class="no-highlight">382</i>&nbsp;                        fileCreated = true;</b>
<b class="nc"><i class="no-highlight">383</i>&nbsp;                    }</b>
<b class="nc"><i class="no-highlight">384</i>&nbsp;                }</b>
<i class="no-highlight">385</i>&nbsp;                writer.write(dataMap);
<b class="nc"><i class="no-highlight">386</i>&nbsp;                document.acknowledge();</b>
<b class="nc"><i class="no-highlight">387</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">388</i>&nbsp;        } catch (BadConfigurationException bce) {</b>
<b class="nc"><i class="no-highlight">389</i>&nbsp;            Throwable rootCause = Throwables.getRootCause(bce);</b>
<b class="nc"><i class="no-highlight">390</i>&nbsp;            SnapDataException failException = new SnapDataException(rootCause, ERR_FORMAT_PARQUET)</b>
<i class="no-highlight">391</i>&nbsp;                    .withReason(bce.getMessage() + Messages.SNAPPLEX_MAYBE_MISSING_DEPENDENCY)
<b class="nc"><i class="no-highlight">392</i>&nbsp;                    .withResolution(Messages.ENSURE_VALID_SNAPLEX_SETUP);</b>
<b class="nc"><i class="no-highlight">393</i>&nbsp;            errorViews.write(failException);</b>
<b class="nc"><i class="no-highlight">394</i>&nbsp;            isBadConfig = true;</b>
<b class="nc"><i class="no-highlight">395</i>&nbsp;        } catch (AmazonClientException e) {</b>
<b class="nc"><i class="no-highlight">396</i>&nbsp;            S3ErrorHandler.invalidEndPoint(((HadoopS3Account) account).endpointName, e);</b>
<b class="nc"><i class="no-highlight">397</i>&nbsp;        } catch (SnapDataException e) {</b>
<b class="nc"><i class="no-highlight">398</i>&nbsp;            errorViews.write(e, lastDocument);</b>
<b class="nc"><i class="no-highlight">399</i>&nbsp;        } catch (SuggestViewAbortException e) {</b>
<b class="nc"><i class="no-highlight">400</i>&nbsp;            throw e;</b>
<i class="no-highlight">401</i>&nbsp;        } catch (Exception e) {
<i class="no-highlight">402</i>&nbsp;            SnapDataException ex = new SnapDataException(e, ERR_PARQUET_WRITE)
<i class="no-highlight">403</i>&nbsp;                    .withResolution(RESOLUTION_ADDRESS_ISSUE);
<i class="no-highlight">404</i>&nbsp;            errorViews.write(ex, lastDocument);
<i class="no-highlight">405</i>&nbsp;        } finally {
<i class="no-highlight">406</i>&nbsp;            try {
<i class="no-highlight">407</i>&nbsp;                IOUtils.closeQuietly(writer);
<i class="no-highlight">408</i>&nbsp;                if (fileCreated &amp;&amp; StringUtils.startsWith(filenameNoAuth, PROTOCOL_ABFS)) {
<b class="nc"><i class="no-highlight">409</i>&nbsp;                    handleParquetOrcWriteToAbfs();</b>
<b class="nc"><i class="no-highlight">410</i>&nbsp;                    deleteTmpFile(tempFile, DELETE_TEMP_FILE_MESSAGE_PARQUET);</b>
<b class="nc"><i class="no-highlight">411</i>&nbsp;                }</b>
<i class="no-highlight">412</i>&nbsp;            } catch (AmazonClientException e) {
<b class="nc"><i class="no-highlight">413</i>&nbsp;                S3ErrorHandler.invalidEndPoint(((HadoopS3Account) account).endpointName, e);</b>
<b class="nc"><i class="no-highlight">414</i>&nbsp;            } catch (Exception e) {</b>
<b class="nc"><i class="no-highlight">415</i>&nbsp;                SnapDataException ex = new SnapDataException(e, ERR_PARQUET_WRITE)</b>
<b class="nc"><i class="no-highlight">416</i>&nbsp;                        .withResolution(RESOLUTION_ADDRESS_ISSUE)</b>
<b class="nc"><i class="no-highlight">417</i>&nbsp;                        .withReason(e.getMessage());</b>
<i class="no-highlight">418</i>&nbsp;                errorViews.write(ex, lastDocument);
<b class="nc"><i class="no-highlight">419</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">420</i>&nbsp;        }</b>
<i class="no-highlight">421</i>&nbsp;    }
<b class="nc"><i class="no-highlight">422</i>&nbsp;</b>
<i class="no-highlight">423</i>&nbsp;    /**
<b class="nc"><i class="no-highlight">424</i>&nbsp;     * Write to multiple, partitioned output files. Only create writers if there&#39;s at</b>
<i class="no-highlight">425</i>&nbsp;     * least one record.
<b class="nc"><i class="no-highlight">426</i>&nbsp;     */</b>
<i class="no-highlight">427</i>&nbsp;    protected void doWorkPartitioned() {
<b class="nc"><i class="no-highlight">428</i>&nbsp;        // multiple Parquet writers, one for each partition</b>
<b class="nc"><i class="no-highlight">429</i>&nbsp;        parquetWriters = new LinkedHashMap&lt;&gt;();</b>
<i class="no-highlight">430</i>&nbsp;        abfsPartitionedFilePathMap = new LinkedHashMap&lt;&gt;();
<i class="no-highlight">431</i>&nbsp;        Document lastDocument = null;
<i class="no-highlight">432</i>&nbsp;        try {
<i class="no-highlight">433</i>&nbsp;            for (Document document : inputViews.documents(inputViews.get(DATA_VIEW_NAME))) {
<i class="no-highlight">434</i>&nbsp;                List&lt;Object&gt; partitionValue = new ArrayList&lt;&gt;();
<b class="nc"><i class="no-highlight">435</i>&nbsp;                lastDocument = document;</b>
<i class="no-highlight">436</i>&nbsp;                if (azureSasUriProperties != null) {
<b class="nc"><i class="no-highlight">437</i>&nbsp;                    extractSasToken(document);</b>
<b class="nc"><i class="no-highlight">438</i>&nbsp;                }</b>
<b class="nc"><i class="no-highlight">439</i>&nbsp;                Map&lt;String, Object&gt; dataMap = document.get(Map.class);</b>
<b class="nc"><i class="no-highlight">440</i>&nbsp;                setPartitionValues(partitionBy, dataMap, partitionValue);</b>
<i class="no-highlight">441</i>&nbsp;                // get partition-by file path
<b class="nc"><i class="no-highlight">442</i>&nbsp;                String filePath = getPartitionFilePath(dataMap);</b>
<b class="nc"><i class="no-highlight">443</i>&nbsp;                // get Parquet writer</b>
<i class="no-highlight">444</i>&nbsp;                ParquetWriter writer = parquetWriters.get(filePath);
<b class="nc"><i class="no-highlight">445</i>&nbsp;                // create Parquet writer if not exists</b>
<b class="nc"><i class="no-highlight">446</i>&nbsp;                if (writer == null) {</b>
<b class="nc"><i class="no-highlight">447</i>&nbsp;                    // If ABFS, write partitioned data into temp files</b>
<b class="nc"><i class="no-highlight">448</i>&nbsp;                    if (StringUtils.startsWithIgnoreCase(filenameNoAuth, PROTOCOL_ABFS)) {</b>
<b class="nc"><i class="no-highlight">449</i>&nbsp;                        setTempFile();</b>
<b class="nc"><i class="no-highlight">450</i>&nbsp;                        // Store temp file paths corresponding to the partition paths and create</b>
<b class="nc"><i class="no-highlight">451</i>&nbsp;                        // a writer with the temp file. The variable &#39;filePath&#39; contains the path</b>
<b class="nc"><i class="no-highlight">452</i>&nbsp;                        // in the format abfs:///[container]/[partitionpath], so that can&#39;t be</b>
<b class="nc"><i class="no-highlight">453</i>&nbsp;                        // used to create the parquet writer. Hence, create writer is called with</b>
<b class="nc"><i class="no-highlight">454</i>&nbsp;                        // the temp file name.</b>
<b class="nc"><i class="no-highlight">455</i>&nbsp;                        AbfsPartitionedFileObject abfsObj =</b>
<b class="nc"><i class="no-highlight">456</i>&nbsp;                                new AbfsPartitionedFileObject(tempFile, filename, partitionDirPath);</b>
<b class="nc"><i class="no-highlight">457</i>&nbsp;                        abfsPartitionedFilePathMap.put(filePath, abfsObj);</b>
<b class="nc"><i class="no-highlight">458</i>&nbsp;                        writer = createParquetWriter(filename);</b>
<b class="nc"><i class="no-highlight">459</i>&nbsp;                    } else {</b>
<b class="nc"><i class="no-highlight">460</i>&nbsp;                        writer = createParquetWriter(filePath);</b>
<i class="no-highlight">461</i>&nbsp;                    }
<b class="nc"><i class="no-highlight">462</i>&nbsp;                    parquetWriters.put(filePath, writer);</b>
<i class="no-highlight">463</i>&nbsp;                    partitionValues.add(partitionValue);
<i class="no-highlight">464</i>&nbsp;                }
<b class="nc"><i class="no-highlight">465</i>&nbsp;                writer.write(dataMap);</b>
<b class="nc"><i class="no-highlight">466</i>&nbsp;                document.acknowledge();</b>
<b class="nc"><i class="no-highlight">467</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">468</i>&nbsp;        } catch (BadConfigurationException bce) {</b>
<i class="no-highlight">469</i>&nbsp;            Throwable rootCause = Throwables.getRootCause(bce);
<i class="no-highlight">470</i>&nbsp;            SnapDataException failException = new SnapDataException(rootCause, ERR_FORMAT_PARQUET)
<i class="no-highlight">471</i>&nbsp;                    .withReason(bce.getMessage() + Messages.SNAPPLEX_MAYBE_MISSING_DEPENDENCY)
<i class="no-highlight">472</i>&nbsp;                    .withResolution(Messages.ENSURE_VALID_SNAPLEX_SETUP);
<b class="nc"><i class="no-highlight">473</i>&nbsp;            errorViews.write(failException);</b>
<b class="nc"><i class="no-highlight">474</i>&nbsp;            isBadConfig = true;</b>
<i class="no-highlight">475</i>&nbsp;        } catch (AmazonClientException e) {
<b class="nc"><i class="no-highlight">476</i>&nbsp;            S3ErrorHandler.invalidEndPoint(((HadoopS3Account) account).endpointName, e);</b>
<b class="nc"><i class="no-highlight">477</i>&nbsp;        } catch (SuggestViewAbortException e) {</b>
<b class="nc"><i class="no-highlight">478</i>&nbsp;            throw e;</b>
<b class="nc"><i class="no-highlight">479</i>&nbsp;        } catch (Exception e) {</b>
<i class="no-highlight">480</i>&nbsp;            errorViews.write(
<b class="nc"><i class="no-highlight">481</i>&nbsp;                    (SnapDataException) new SnapDataException(e, ERR_FORMAT_PARQUET)</b>
<b class="nc"><i class="no-highlight">482</i>&nbsp;                            .withReason(e.getMessage()).withResolution(RESOLUTION_ADDRESS_ISSUE),</b>
<i class="no-highlight">483</i>&nbsp;                    lastDocument);
<b class="nc"><i class="no-highlight">484</i>&nbsp;        } finally {</b>
<b class="nc"><i class="no-highlight">485</i>&nbsp;            try {</b>
<b class="nc"><i class="no-highlight">486</i>&nbsp;                for (ParquetWriter parquetWriter : parquetWriters.values()) {</b>
<b class="nc"><i class="no-highlight">487</i>&nbsp;                    IOUtils.closeQuietly(parquetWriter);</b>
<i class="no-highlight">488</i>&nbsp;                }
<i class="no-highlight">489</i>&nbsp;                /* TODO: Right now, temp files are created for each partition and this may result
<i class="no-highlight">490</i>&nbsp;                   in space availability issues. Will have to try a different approach to avoid
<i class="no-highlight">491</i>&nbsp;                   the space bottleneck. */
<i class="no-highlight">492</i>&nbsp;                // If ABFS, handle differently as parquet files were written to temp files first
<b class="nc"><i class="no-highlight">493</i>&nbsp;                if (StringUtils.startsWithIgnoreCase(filenameNoAuth, PROTOCOL_ABFS)) {</b>
<b class="nc"><i class="no-highlight">494</i>&nbsp;                    handlePartitionedWritesToAbfs();</b>
<b class="nc"><i class="no-highlight">495</i>&nbsp;                }</b>
<i class="no-highlight">496</i>&nbsp;            } catch (AmazonClientException e) {
<i class="no-highlight">497</i>&nbsp;                S3ErrorHandler.invalidEndPoint(((HadoopS3Account) account).endpointName, e);
<i class="no-highlight">498</i>&nbsp;            } catch (Exception e) {
<i class="no-highlight">499</i>&nbsp;                errorViews.write(
<i class="no-highlight">500</i>&nbsp;                        (SnapDataException) new SnapDataException(e, ERR_PARQUET_WRITE)
<i class="no-highlight">501</i>&nbsp;                                .withReason(e.getMessage())
<b class="nc"><i class="no-highlight">502</i>&nbsp;                                .withResolution(RESOLUTION_ADDRESS_ISSUE), lastDocument);</b>
<i class="no-highlight">503</i>&nbsp;            } finally {
<b class="nc"><i class="no-highlight">504</i>&nbsp;                IOUtils.closeQuietly(inputStream);</b>
<b class="nc"><i class="no-highlight">505</i>&nbsp;                cleanup();</b>
<b class="nc"><i class="no-highlight">506</i>&nbsp;            }</b>
<i class="no-highlight">507</i>&nbsp;        }
<i class="no-highlight">508</i>&nbsp;    }
<i class="no-highlight">509</i>&nbsp;
<b class="nc"><i class="no-highlight">510</i>&nbsp;    private void setPartitionValues(final List&lt;String&gt; partitionBy,</b>
<b class="nc"><i class="no-highlight">511</i>&nbsp;                                    final Map&lt;String, Object&gt; dataMap,</b>
<b class="nc"><i class="no-highlight">512</i>&nbsp;                                    final List&lt;Object&gt; partitionValue) {</b>
<b class="nc"><i class="no-highlight">513</i>&nbsp;        partitionBy.forEach(key -&gt; {</b>
<b class="nc"><i class="no-highlight">514</i>&nbsp;            if (dataMap.containsKey(key)) {</b>
<b class="nc"><i class="no-highlight">515</i>&nbsp;                partitionValue.add(dataMap.get(key));</b>
<i class="no-highlight">516</i>&nbsp;            }
<i class="no-highlight">517</i>&nbsp;        });
<i class="no-highlight">518</i>&nbsp;    }
<b class="nc"><i class="no-highlight">519</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">520</i>&nbsp;    @Override</b>
<i class="no-highlight">521</i>&nbsp;    protected void doWork() {
<b class="nc"><i class="no-highlight">522</i>&nbsp;        isExecuteDuringPreview = true;</b>
<b class="nc"><i class="no-highlight">523</i>&nbsp;        try {</b>
<b class="nc"><i class="no-highlight">524</i>&nbsp;            user.doAs((PrivilegedAction&lt;Void&gt;) () -&gt; {</b>
<b class="nc"><i class="no-highlight">525</i>&nbsp;                doWorkImpl();</b>
<i class="no-highlight">526</i>&nbsp;                return null;
<b class="nc"><i class="no-highlight">527</i>&nbsp;            });</b>
<i class="no-highlight">528</i>&nbsp;        } finally {
<b class="nc"><i class="no-highlight">529</i>&nbsp;            // make sure all input documents are consumed</b>
<b class="nc"><i class="no-highlight">530</i>&nbsp;            for (InputView inputView : inputViews) {</b>
<b class="nc"><i class="no-highlight">531</i>&nbsp;                for (Document document : inputViews.documents(inputView)) {</b>
<i class="no-highlight">532</i>&nbsp;                    document.acknowledge();
<b class="nc"><i class="no-highlight">533</i>&nbsp;                }</b>
<i class="no-highlight">534</i>&nbsp;            }
<i class="no-highlight">535</i>&nbsp;        }
<i class="no-highlight">536</i>&nbsp;    }
<i class="no-highlight">537</i>&nbsp;
<i class="no-highlight">538</i>&nbsp;    private void doWorkImpl() {
<i class="no-highlight">539</i>&nbsp;        if (!inputViews.isEmpty()) {
<b class="nc"><i class="no-highlight">540</i>&nbsp;            if (schema == null) {</b>
<i class="no-highlight">541</i>&nbsp;                try {
<b class="nc"><i class="no-highlight">542</i>&nbsp;                    schema = getSchemaFromSchemaView();</b>
<b class="nc"><i class="no-highlight">543</i>&nbsp;                } catch (SnapDataException e) {</b>
<i class="no-highlight">544</i>&nbsp;                    errorViews.write(e);
<i class="no-highlight">545</i>&nbsp;                    isExecuteDuringPreview = false;
<b class="nc"><i class="no-highlight">546</i>&nbsp;                    return;</b>
<i class="no-highlight">547</i>&nbsp;                }
<b class="nc"><i class="no-highlight">548</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">549</i>&nbsp;            initializeAbfsCallableRestUtility();</b>
<b class="nc"><i class="no-highlight">550</i>&nbsp;            if (CollectionUtils.isEmpty(partitionBy)) {</b>
<b class="nc"><i class="no-highlight">551</i>&nbsp;                doWorkUnpartitioned();</b>
<b class="nc"><i class="no-highlight">552</i>&nbsp;            } else {</b>
<b class="nc"><i class="no-highlight">553</i>&nbsp;                doWorkPartitioned();</b>
<b class="nc"><i class="no-highlight">554</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">555</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">556</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">557</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">558</i>&nbsp;    @Override</b>
<b class="nc"><i class="no-highlight">559</i>&nbsp;    public void handle(final LifecycleEvent event) {</b>
<b class="nc"><i class="no-highlight">560</i>&nbsp;        switch (event) {</b>
<b class="nc"><i class="no-highlight">561</i>&nbsp;            case SUCCESS:</b>
<b class="nc"><i class="no-highlight">562</i>&nbsp;                if (isBadConfig ||</b>
<b class="nc"><i class="no-highlight">563</i>&nbsp;                        (!fileCreated &amp;&amp; (parquetWriters == null || parquetWriters.isEmpty()))) {</b>
<b class="nc"><i class="no-highlight">564</i>&nbsp;                    return;</b>
<i class="no-highlight">565</i>&nbsp;                }
<i class="no-highlight">566</i>&nbsp;                if (!inputViews.isEmpty() &amp;&amp; inputViews.get(DATA_VIEW_NAME).getObjectCount() != 0) {
<i class="no-highlight">567</i>&nbsp;                    // If output view exists, then write file name
<i class="no-highlight">568</i>&nbsp;                    if (isExecuteDuringPreview &amp;&amp; !outputViews.isEmpty()) {
<i class="no-highlight">569</i>&nbsp;                        generateSchema();
<i class="no-highlight">570</i>&nbsp;                        if (MapUtils.isEmpty(parquetWriters)) {
<i class="no-highlight">571</i>&nbsp;                            Map&lt;String, Object&gt; data = new LinkedHashMap&lt;&gt;();
<i class="no-highlight">572</i>&nbsp;                            data.put(FILENAME, filenameNoAuth);
<i class="no-highlight">573</i>&nbsp;                            data.put(SCHEMA, parquetSchema);
<i class="no-highlight">574</i>&nbsp;                            outputViews.write(documentUtility.newDocument(data));
<i class="no-highlight">575</i>&nbsp;                        } else {
<i class="no-highlight">576</i>&nbsp;                            int docCounter = 0;
<b class="nc"><i class="no-highlight">577</i>&nbsp;                            for (String path : parquetWriters.keySet()) {</b>
<b class="nc"><i class="no-highlight">578</i>&nbsp;                                Map&lt;String, Object&gt; data = new LinkedHashMap&lt;&gt;();</b>
<b class="nc"><i class="no-highlight">579</i>&nbsp;                              data.put(FILENAME, path);</b>
<i class="no-highlight">580</i>&nbsp;                                data.put(SCHEMA, parquetSchema);
<i class="no-highlight">581</i>&nbsp;                                data.put(PARTITION_KEY, partitionBy);
<i class="no-highlight">582</i>&nbsp;                              data.put(PARTITION_VALUE, partitionValues.get(docCounter++));
<i class="no-highlight">583</i>&nbsp;                                outputViews.write(documentUtility.newDocument(data));
<i class="no-highlight">584</i>&nbsp;                            }
<b class="nc"><i class="no-highlight">585</i>&nbsp;                        }</b>
<i class="no-highlight">586</i>&nbsp;                    }
<i class="no-highlight">587</i>&nbsp;                }
<i class="no-highlight">588</i>&nbsp;        }
<i class="no-highlight">589</i>&nbsp;    }
<b class="nc"><i class="no-highlight">590</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">591</i>&nbsp;    @Override</b>
<i class="no-highlight">592</i>&nbsp;    public void cleanup() throws ExecutionException {
<i class="no-highlight">593</i>&nbsp;    }
<i class="no-highlight">594</i>&nbsp;
<i class="no-highlight">595</i>&nbsp;    @Override
<i class="no-highlight">596</i>&nbsp;    public void defineViews(final ViewBuilder viewBuilder) {
<i class="no-highlight">597</i>&nbsp;        viewBuilder.describe(DATA_VIEW_NAME, DATA_INPUT_VIEW_LABEL, DATA_INPUT_VIEW_DESC)
<i class="no-highlight">598</i>&nbsp;                .type(ViewType.DOCUMENT)
<b class="nc"><i class="no-highlight">599</i>&nbsp;                .add(ViewCategory.INPUT);</b>
<b class="nc"><i class="no-highlight">600</i>&nbsp;    }</b>
<i class="no-highlight">601</i>&nbsp;
<i class="no-highlight">602</i>&nbsp;    @Override
<i class="no-highlight">603</i>&nbsp;    public void configureForSuggest(final PropertyValues propertyValues,
<i class="no-highlight">604</i>&nbsp;            final BigInteger maxSuggestValue) throws ConfigurationException {
<i class="no-highlight">605</i>&nbsp;        configure(propertyValues);
<i class="no-highlight">606</i>&nbsp;    }
<i class="no-highlight">607</i>&nbsp;
<i class="no-highlight">608</i>&nbsp;    @Override
<i class="no-highlight">609</i>&nbsp;    public void executeForSuggest() throws ExecutionException {
<i class="no-highlight">610</i>&nbsp;        if (isExecuteDuringPreview) {
<i class="no-highlight">611</i>&nbsp;            execute();
<i class="no-highlight">612</i>&nbsp;        }
<b class="nc"><i class="no-highlight">613</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">614</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">615</i>&nbsp;    /**</b>
<b class="nc"><i class="no-highlight">616</i>&nbsp;     * These values are used to keep UI consistent with other snap&#39;s compression codec</b>
<i class="no-highlight">617</i>&nbsp;     * field.
<b class="nc"><i class="no-highlight">618</i>&nbsp;     */</b>
<i class="no-highlight">619</i>&nbsp;    private enum SupportedCodecs {
<b class="nc"><i class="no-highlight">620</i>&nbsp;        NONE, SNAPPY, GZIP, LZO;</b>
<b class="nc"><i class="no-highlight">621</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">622</i>&nbsp;        private static final String UNCOMPRESSED = &quot;uncompressed&quot;;</b>
<i class="no-highlight">623</i>&nbsp;
<i class="no-highlight">624</i>&nbsp;        /**
<i class="no-highlight">625</i>&nbsp;         * To match other snap compression settings, the value of NONE should be mapped to the
<i class="no-highlight">626</i>&nbsp;         * UNCOMPRESSED value.
<i class="no-highlight">627</i>&nbsp;         *
<i class="no-highlight">628</i>&nbsp;         * @param compression the configured compression
<b class="nc"><i class="no-highlight">629</i>&nbsp;         * @return the CompressionCodecName equivalent of this enum</b>
<b class="nc"><i class="no-highlight">630</i>&nbsp;         */</b>
<b class="nc"><i class="no-highlight">631</i>&nbsp;        public static CompressionCodecName toCompressionCodecName(String compression) {</b>
<i class="no-highlight">632</i>&nbsp;            try {
<b class="nc"><i class="no-highlight">633</i>&nbsp;                if (compression.toLowerCase().equals(UNCOMPRESSED)) {</b>
<i class="no-highlight">634</i>&nbsp;                    return CompressionCodecName.UNCOMPRESSED;
<b class="nc"><i class="no-highlight">635</i>&nbsp;                } else if (valueOf(compression) == NONE) {</b>
<b class="nc"><i class="no-highlight">636</i>&nbsp;                    return CompressionCodecName.UNCOMPRESSED;</b>
<i class="no-highlight">637</i>&nbsp;                } else {
<i class="no-highlight">638</i>&nbsp;                    return CompressionCodecName.valueOf(compression);
<b class="nc"><i class="no-highlight">639</i>&nbsp;                }</b>
<b class="nc"><i class="no-highlight">640</i>&nbsp;            } catch (IllegalArgumentException e) {</b>
<b class="nc"><i class="no-highlight">641</i>&nbsp;                throw new ConfigurationException(e, PARQUET_INVALID_CODEC)</b>
<i class="no-highlight">642</i>&nbsp;                        .withResolution(PARQUET_CHOOSE_CODEC);
<i class="no-highlight">643</i>&nbsp;            }
<b class="nc"><i class="no-highlight">644</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">645</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">646</i>&nbsp;</b>
<i class="no-highlight">647</i>&nbsp;    private ParquetWriter createParquetWriter(String filename)
<b class="nc"><i class="no-highlight">648</i>&nbsp;            throws AmazonClientException {</b>
<b class="nc"><i class="no-highlight">649</i>&nbsp;        Path path = new Path(getUriFor(filename));</b>
<b class="nc"><i class="no-highlight">650</i>&nbsp;        Configuration configuration = configurationSupplier.get();</b>
<i class="no-highlight">651</i>&nbsp;        HadoopConfigUtils.adjustConfig(configuration, account);
<b class="nc"><i class="no-highlight">652</i>&nbsp;        ParquetWriter parquetWriter;</b>
<i class="no-highlight">653</i>&nbsp;        LOG.debug(HADOOP_CONFIG_LOG, configToMap(configuration));
<b class="nc"><i class="no-highlight">654</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">655</i>&nbsp;        NewHadoopPathAndConfig newHadoopPathAndConfig = SasTokenUtils</b>
<b class="nc"><i class="no-highlight">656</i>&nbsp;                .addSasTokenIfRequiredAndGetNewPathAndConfig(configuration, filename,</b>
<b class="nc"><i class="no-highlight">657</i>&nbsp;                        sasTokenFromSnap, sasTokenFromAccount, account, authTypeSasUri,</b>
<b class="nc"><i class="no-highlight">658</i>&nbsp;                        accountName);</b>
<i class="no-highlight">659</i>&nbsp;        if (newHadoopPathAndConfig != null) {
<i class="no-highlight">660</i>&nbsp;            path = newHadoopPathAndConfig.getPath();
<b class="nc"><i class="no-highlight">661</i>&nbsp;            configuration = newHadoopPathAndConfig.getConfiguration();</b>
<b class="nc"><i class="no-highlight">662</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">663</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">664</i>&nbsp;        if (account instanceof KerberosAccount) {</b>
<b class="nc"><i class="no-highlight">665</i>&nbsp;            KerberosConfig krb5Config = ((KerberosAccount) account).getKerberosConfig();</b>
<b class="nc"><i class="no-highlight">666</i>&nbsp;            HdfsUtils.updateConfWithKerberosAuthentication(configuration);</b>
<b class="nc"><i class="no-highlight">667</i>&nbsp;            try {</b>
<b class="nc"><i class="no-highlight">668</i>&nbsp;                final Path parquetPath = path;</b>
<i class="no-highlight">669</i>&nbsp;                final Configuration parquetConfig = configuration;
<i class="no-highlight">670</i>&nbsp;                parquetWriter = KerberosAuthentication.runHadoopPrivilegedAction(
<b class="nc"><i class="no-highlight">671</i>&nbsp;                        () -&gt; {</b>
<b class="nc"><i class="no-highlight">672</i>&nbsp;                            ParquetWriter parquetWriter1 = new</b>
<b class="nc"><i class="no-highlight">673</i>&nbsp;                                    ParquetWriterBuilder(parquetPath, schema, roundingMode)</b>
<b class="nc"><i class="no-highlight">674</i>&nbsp;                                    .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)</b>
<b class="nc"><i class="no-highlight">675</i>&nbsp;                                    .withCompressionCodec(codec)</b>
<b class="nc"><i class="no-highlight">676</i>&nbsp;                                    .withConf(parquetConfig)</b>
<b class="nc"><i class="no-highlight">677</i>&nbsp;                                    .build();</b>
<b class="nc"><i class="no-highlight">678</i>&nbsp;                            return parquetWriter1;</b>
<i class="no-highlight">679</i>&nbsp;                        }
<i class="no-highlight">680</i>&nbsp;                        , krb5Config);
<b class="nc"><i class="no-highlight">681</i>&nbsp;            } catch (IOException ex) {</b>
<b class="nc"><i class="no-highlight">682</i>&nbsp;                String reason = String.format(FAILED_TO_READ_KERBEROS_INCOMING_DATA, ex</b>
<b class="nc"><i class="no-highlight">683</i>&nbsp;                        .getMessage());</b>
<b class="nc"><i class="no-highlight">684</i>&nbsp;                throw new ExecutionException(ex, UNABLE_TO_READ_INPUT_STREAM)</b>
<i class="no-highlight">685</i>&nbsp;                        .withReason(reason)
<i class="no-highlight">686</i>&nbsp;                        .withResolution(ERR_RESOLUTION_INVALIDINPUT);
<i class="no-highlight">687</i>&nbsp;            }
<i class="no-highlight">688</i>&nbsp;        } else {
<i class="no-highlight">689</i>&nbsp;            //for non-kerberos filesystem
<b class="nc"><i class="no-highlight">690</i>&nbsp;            try {</b>
<i class="no-highlight">691</i>&nbsp;                parquetWriter = new ParquetWriterBuilder(path, schema, roundingMode)
<b class="nc"><i class="no-highlight">692</i>&nbsp;                                        .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)</b>
<b class="nc"><i class="no-highlight">693</i>&nbsp;                                        .withCompressionCodec(codec)</b>
<i class="no-highlight">694</i>&nbsp;                                        .withConf(configuration)
<b class="nc"><i class="no-highlight">695</i>&nbsp;                                        .build();</b>
<b class="nc"><i class="no-highlight">696</i>&nbsp;            } catch (SuggestViewAbortException e) {</b>
<b class="nc"><i class="no-highlight">697</i>&nbsp;                throw e;</b>
<b class="nc"><i class="no-highlight">698</i>&nbsp;            } catch (NullPointerException e) {</b>
<b class="nc"><i class="no-highlight">699</i>&nbsp;                // It was observed that Hadoop library threw NPE when the given wasb container</b>
<i class="no-highlight">700</i>&nbsp;                // was not found.
<b class="nc"><i class="no-highlight">701</i>&nbsp;                throw new SnapDataException(e, ERR_INIT_PARQUET_WRITER)</b>
<i class="no-highlight">702</i>&nbsp;                              .withReason(REASON_INIT_PARQUET_WRITER)
<i class="no-highlight">703</i>&nbsp;                              .withResolution(RESOLUTION_ADDRESS_ISSUE);
<i class="no-highlight">704</i>&nbsp;            } catch (Exception e) {
<b class="nc"><i class="no-highlight">705</i>&nbsp;                // It was observed that Hadoop library threw NPE when the given wasb container did</b>
<b class="nc"><i class="no-highlight">706</i>&nbsp;                // not exist or IOException when no account was given and it failed to get schema.</b>
<i class="no-highlight">707</i>&nbsp;                /*
<b class="nc"><i class="no-highlight">708</i>&nbsp;                We will try to go deep down to the root cause, and get its message.</b>
<b class="nc"><i class="no-highlight">709</i>&nbsp;                 */</b>
<b class="nc"><i class="no-highlight">710</i>&nbsp;                Throwable rootCause = e;</b>
<b class="nc"><i class="no-highlight">711</i>&nbsp;                Throwable nextCause;</b>
<b class="nc"><i class="no-highlight">712</i>&nbsp;                while ((nextCause = rootCause.getCause()) != null) {</b>
<b class="nc"><i class="no-highlight">713</i>&nbsp;                    rootCause = nextCause;</b>
<b class="nc"><i class="no-highlight">714</i>&nbsp;                }</b>
<i class="no-highlight">715</i>&nbsp;                throw new SnapDataException(e, ERR_INIT_PARQUET_WRITER)
<i class="no-highlight">716</i>&nbsp;                        .withReason(String.format(REASON_INIT_PARQUET_WRITER_WITH_DETAIL,
<i class="no-highlight">717</i>&nbsp;                                rootCause.getMessage()))
<b class="nc"><i class="no-highlight">718</i>&nbsp;                        .withResolution(RESOLUTION_ADDRESS_ISSUE);</b>
<b class="nc"><i class="no-highlight">719</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">720</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">721</i>&nbsp;        return parquetWriter;</b>
<b class="nc"><i class="no-highlight">722</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">723</i>&nbsp;</b>
<i class="no-highlight">724</i>&nbsp;    private void generateSchema() {
<b class="nc"><i class="no-highlight">725</i>&nbsp;        SchemaConverter schemaConverter = new SchemaConverter(schemaFactory);</b>
<b class="nc"><i class="no-highlight">726</i>&nbsp;        ObjectSchema objectSchema = schemaConverter.convert(schema);</b>
<b class="nc"><i class="no-highlight">727</i>&nbsp;        try {</b>
<b class="nc"><i class="no-highlight">728</i>&nbsp;            parquetSchema = snapObjectMapper.readValue(objectSchema.toString(), Map.class);</b>
<b class="nc"><i class="no-highlight">729</i>&nbsp;        } catch (IOException e) {</b>
<i class="no-highlight">730</i>&nbsp;            Throwable t = com.google.common.base.Throwables.getRootCause(e);
<i class="no-highlight">731</i>&nbsp;            throw new SnapDataException(t, PARQUET_INVALID_SCHEMA)
<i class="no-highlight">732</i>&nbsp;                    .withReason(e.getMessage())
<b class="nc"><i class="no-highlight">733</i>&nbsp;                    .withResolution(RESOLUTION_ADDRESS_ISSUE);</b>
<b class="nc"><i class="no-highlight">734</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">735</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">736</i>&nbsp;</b>
<i class="no-highlight">737</i>&nbsp;    private String getPartitionFilePath(Map&lt;String, Object&gt; data) {
<b class="nc"><i class="no-highlight">738</i>&nbsp;        StringBuilder builder = new StringBuilder();</b>
<b class="nc"><i class="no-highlight">739</i>&nbsp;        for (String key : partitionBy) {</b>
<i class="no-highlight">740</i>&nbsp;            String partitionByFolder = validatePartitionByFolder(data.get(key), key);
<i class="no-highlight">741</i>&nbsp;            builder.append(URL_SEPARATOR)
<i class="no-highlight">742</i>&nbsp;                    .append(partitionByFolder);
<i class="no-highlight">743</i>&nbsp;        }
<i class="no-highlight">744</i>&nbsp;        // Store the new partition dir path to create the corresponding directory in ABFS
<i class="no-highlight">745</i>&nbsp;        partitionDirPath = builder.toString();
<i class="no-highlight">746</i>&nbsp;        int index = filenameNoAuth.lastIndexOf(URL_SEPARATOR);
<b class="nc"><i class="no-highlight">747</i>&nbsp;        String filePath = StringUtils.join(filenameNoAuth.substring(0, index), builder.toString(),</b>
<b class="nc"><i class="no-highlight">748</i>&nbsp;                filenameNoAuth.substring(index));</b>
<b class="nc"><i class="no-highlight">749</i>&nbsp;        return filePath;</b>
<b class="nc"><i class="no-highlight">750</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">751</i>&nbsp;</b>
<i class="no-highlight">752</i>&nbsp;    private String validatePartitionByFolder(Object object, String key) {
<i class="no-highlight">753</i>&nbsp;        if (object instanceof String) {
<b class="nc"><i class="no-highlight">754</i>&nbsp;            return (String) object;</b>
<b class="nc"><i class="no-highlight">755</i>&nbsp;        } else if (object instanceof Number) {</b>
<b class="nc"><i class="no-highlight">756</i>&nbsp;            return object.toString();</b>
<b class="nc"><i class="no-highlight">757</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">758</i>&nbsp;        throw new SnapDataException(ERR_PARTITION_BY_SUBDIRECTORY)</b>
<b class="nc"><i class="no-highlight">759</i>&nbsp;                .formatWith(key);</b>
<b class="nc"><i class="no-highlight">760</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">761</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">762</i>&nbsp;    /**</b>
<i class="no-highlight">763</i>&nbsp;     * Generate the parquet schema from the schema InputView. The expected input structure is
<b class="nc"><i class="no-highlight">764</i>&nbsp;     * expected to be the same as the result of &quot;Describe table&quot; statement in HiveQL</b>
<i class="no-highlight">765</i>&nbsp;     */
<b class="nc"><i class="no-highlight">766</i>&nbsp;    protected MessageType getSchemaFromSchemaView() {</b>
<b class="nc"><i class="no-highlight">767</i>&nbsp;        InputView schemaView = inputViews.get(schemaViewName);</b>
<b class="nc"><i class="no-highlight">768</i>&nbsp;        List&lt;String&gt; columns = Lists.newArrayList();</b>
<i class="no-highlight">769</i>&nbsp;        List&lt;TypeInfo&gt; types = Lists.newArrayList();
<b class="nc"><i class="no-highlight">770</i>&nbsp;        for (Document doc :  inputViews.documents(schemaView)) {</b>
<b class="nc"><i class="no-highlight">771</i>&nbsp;            Map&lt;String, String&gt; colMeta = doc.get(Map.class);</b>
<b class="nc"><i class="no-highlight">772</i>&nbsp;            // Describe command for Hive partitioned table can have col_name field empty or</b>
<i class="no-highlight">773</i>&nbsp;            // comments. Skipping processing those documents.
<b class="nc"><i class="no-highlight">774</i>&nbsp;            String colName = colMeta.get(COL_NAME_KEY);</b>
<b class="nc"><i class="no-highlight">775</i>&nbsp;            if (Objects.isNull(colName)) {</b>
<b class="nc"><i class="no-highlight">776</i>&nbsp;                throw new SnapDataException(ERR_COL_NAME_SCHEMA)</b>
<i class="no-highlight">777</i>&nbsp;                        .withReason(REASON_COL_NAME_SCHEMA)
<i class="no-highlight">778</i>&nbsp;                        .withResolution(RESOLUTION_ADDRESS_ISSUE);
<i class="no-highlight">779</i>&nbsp;            } else if (StringUtils.isBlank(colName) ||
<i class="no-highlight">780</i>&nbsp;                    colName.startsWith(COMMENT_PREFIX) ||
<i class="no-highlight">781</i>&nbsp;                    columns.contains(colName)) {
<i class="no-highlight">782</i>&nbsp;                continue;
<i class="no-highlight">783</i>&nbsp;            } else {
<i class="no-highlight">784</i>&nbsp;                columns.add(colName);
<i class="no-highlight">785</i>&nbsp;            }
<i class="no-highlight">786</i>&nbsp;            if (colMeta.get(DATA_TYPE_KEY) != null) {
<i class="no-highlight">787</i>&nbsp;                types.add(TypeInfoUtils.getTypeInfoFromTypeString(
<i class="no-highlight">788</i>&nbsp;                        colMeta.get(DATA_TYPE_KEY).trim()));
<b class="nc"><i class="no-highlight">789</i>&nbsp;            } else {</b>
<i class="no-highlight">790</i>&nbsp;                throw new SnapDataException(ERR_DATA_TYPE_SCHEMA)
<b class="nc"><i class="no-highlight">791</i>&nbsp;                        .withReason(REASON_DATA_TYPE_SCHEMA)</b>
<i class="no-highlight">792</i>&nbsp;                        .withResolution(RESOLUTION_ADDRESS_ISSUE);
<b class="nc"><i class="no-highlight">793</i>&nbsp;            }</b>
<b class="nc"><i class="no-highlight">794</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">795</i>&nbsp;        return MessageTypeParser.parseMessageType(HiveSchemaConverter.convert(columns,</b>
<i class="no-highlight">796</i>&nbsp;                types).toString());
<i class="no-highlight">797</i>&nbsp;
<b class="nc"><i class="no-highlight">798</i>&nbsp;    }</b>
<b class="nc"><i class="no-highlight">799</i>&nbsp;</b>
<b class="nc"><i class="no-highlight">800</i>&nbsp;    /**</b>
<b class="nc"><i class="no-highlight">801</i>&nbsp;     * Iterates through the set of temp files and partition directory paths,</b>
<i class="no-highlight">802</i>&nbsp;     * creates an input stream from the temp file to which the parquet writer
<i class="no-highlight">803</i>&nbsp;     * wrote the data, and uses the partition directory path to create a directory
<i class="no-highlight">804</i>&nbsp;     * in ABFS, and creates the corresponding partitioned .parquet file in the specified path
<b class="nc"><i class="no-highlight">805</i>&nbsp;     * @throws Exception</b>
<b class="nc"><i class="no-highlight">806</i>&nbsp;     */</b>
<b class="nc"><i class="no-highlight">807</i>&nbsp;    private void handlePartitionedWritesToAbfs() throws Exception {</b>
<i class="no-highlight">808</i>&nbsp;        // Save a copy of the original [directory prop value + file prop value]
<i class="no-highlight">809</i>&nbsp;        String filenameNoAuthCopy = filenameNoAuth;
<i class="no-highlight">810</i>&nbsp;        for (Map.Entry&lt;String, AbfsPartitionedFileObject&gt; abfsFilePathEntry :
<i class="no-highlight">811</i>&nbsp;                abfsPartitionedFilePathMap.entrySet()) {
<i class="no-highlight">812</i>&nbsp;            // create new directory path based on the partitioned path
<b class="nc"><i class="no-highlight">813</i>&nbsp;            String dirPath = StringUtils.substringBeforeLast(filenameNoAuthCopy, URL_SEPARATOR)</b>
<b class="nc"><i class="no-highlight">814</i>&nbsp;                    + abfsFilePathEntry.getValue().directory;</b>
<b class="nc"><i class="no-highlight">815</i>&nbsp;            String fileSubstring = StringUtils.substringAfterLast(filenameNoAuthCopy,</b>
<i class="no-highlight">816</i>&nbsp;                    URL_SEPARATOR);
<i class="no-highlight">817</i>&nbsp;            // this is the path sent to ABFS REST Api
<i class="no-highlight">818</i>&nbsp;            filenameNoAuth = dirPath + URL_SEPARATOR + fileSubstring;
<b class="nc"><i class="no-highlight">819</i>&nbsp;            createDirectory(dirPath);</b>
<b class="nc"><i class="no-highlight">820</i>&nbsp;            if (!abfsRestUtility.isCreateDir()) {</b>
<b class="nc"><i class="no-highlight">821</i>&nbsp;                handlingAbfsFailures();</b>
<i class="no-highlight">822</i>&nbsp;                return;
<i class="no-highlight">823</i>&nbsp;            }
<i class="no-highlight">824</i>&nbsp;            // this is used to create the input stream
<i class="no-highlight">825</i>&nbsp;            filename = abfsFilePathEntry.getValue().tmpFileUrl;
<i class="no-highlight">826</i>&nbsp;            initInputStream();
<i class="no-highlight">827</i>&nbsp;            writeDataToAbfs(inputStream);
<i class="no-highlight">828</i>&nbsp;            /* As we iterate through the map, input stream needs to closed, unassigned and
<i class="no-highlight">829</i>&nbsp;               created again for the next temp file. Not doing so will cause bytesRead = -1
<i class="no-highlight">830</i>&nbsp;               as it still would be an input stream of the previous file. Url connection needs to be
<i class="no-highlight">831</i>&nbsp;               disconnected as well and new connection created with the next temp file.
<b class="nc"><i class="no-highlight">832</i>&nbsp;             */</b>
<b class="nc"><i class="no-highlight">833</i>&nbsp;            cleanupAfterEveryPartitionWrite();</b>
<b class="nc"><i class="no-highlight">834</i>&nbsp;            deleteTmpFile(abfsFilePathEntry.getValue().tmpFile, DELETE_TEMP_FILE_MESSAGE_PARQUET);</b>
<b class="nc"><i class="no-highlight">835</i>&nbsp;        }</b>
<i class="no-highlight">836</i>&nbsp;    }
<i class="no-highlight">837</i>&nbsp;
<i class="no-highlight">838</i>&nbsp;    private void cleanupAfterEveryPartitionWrite() {
<i class="no-highlight">839</i>&nbsp;        cleanUpInputStream(inputStream);
<i class="no-highlight">840</i>&nbsp;        inputStream = null;
<i class="no-highlight">841</i>&nbsp;        disconnectUrlConnection();
<i class="no-highlight">842</i>&nbsp;    }
<i class="no-highlight">843</i>&nbsp;
<i class="no-highlight">844</i>&nbsp;    /**
<i class="no-highlight">845</i>&nbsp;     * Class to store the temp file path and the corresponding partition directory
<i class="no-highlight">846</i>&nbsp;     */
<i class="no-highlight">847</i>&nbsp;    class AbfsPartitionedFileObject {
<i class="no-highlight">848</i>&nbsp;        private String tmpFileUrl;
<i class="no-highlight">849</i>&nbsp;        private String directory;
<i class="no-highlight">850</i>&nbsp;        private File tmpFile;
<i class="no-highlight">851</i>&nbsp;
<i class="no-highlight">852</i>&nbsp;        AbfsPartitionedFileObject(File tmpFile, String tmpFileUrl, String directory) {
<i class="no-highlight">853</i>&nbsp;            this.tmpFile = tmpFile;
<i class="no-highlight">854</i>&nbsp;            this.tmpFileUrl = tmpFileUrl;
<i class="no-highlight">855</i>&nbsp;            this.directory = directory;
<i class="no-highlight">856</i>&nbsp;        }
<i class="no-highlight">857</i>&nbsp;    }
<i class="no-highlight">858</i>&nbsp;}
</div>
</pre>
</div>

<script type="text/javascript">
(function() {
    var msie = false, msie9 = false;
    /*@cc_on
      msie = true;
      @if (@_jscript_version >= 9)
        msie9 = true;
      @end
    @*/

    if (!msie || msie && msie9) {
        var codeBlock = document.getElementById('sourceCode');

        if (codeBlock) {
            hljs.highlightBlock(codeBlock);
        }
    }
})();
</script>

<div class="footer">
    
    <div style="float:right;">generated on 2024-01-19 15:49</div>
</div>
</body>
</html>
