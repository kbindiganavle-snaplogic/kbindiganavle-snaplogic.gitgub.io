


<!DOCTYPE html>
<html id="htmlId">
<head>
  <title>Coverage Report > SparkSQLOperations</title>
  <style type="text/css">
    @import "../../css/coverage.css";
    @import "../../css/highlight-idea.css";
  </style>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
</head>

<body>
<div class="content">
<div class="breadCrumbs">
Current scope:     <a href="../../index.html">all classes</a>
    <span class="separator">|</span>
    <a href="../index.html">com.snaplogic.snap.api.sql.operations</a>
</div>

<h1>Coverage Summary for Class: SparkSQLOperations (com.snaplogic.snap.api.sql.operations)</h1>

<table class="coverageStats">
<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Class, %
</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">SparkSQLOperations</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/6)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/41)
  </span>
</td>
</tr>

</table>

<br/>
<br/>


<pre>
<div class="sourceCode" id="sourceCode"><i class="no-highlight">1</i>&nbsp;/*
<i class="no-highlight">2</i>&nbsp; * SnapLogic - Data Integration
<i class="no-highlight">3</i>&nbsp; *
<i class="no-highlight">4</i>&nbsp; * Copyright (C) 2019 - 2021, SnapLogic, Inc.  All rights reserved.
<i class="no-highlight">5</i>&nbsp; *
<i class="no-highlight">6</i>&nbsp; * This program is licensed under the terms of
<i class="no-highlight">7</i>&nbsp; * the SnapLogic Commercial Subscription agreement.
<i class="no-highlight">8</i>&nbsp; *
<i class="no-highlight">9</i>&nbsp; * &quot;SnapLogic&quot; is a trademark of SnapLogic, Inc.
<i class="no-highlight">10</i>&nbsp; */
<i class="no-highlight">11</i>&nbsp;package com.snaplogic.snap.api.sql.operations;
<i class="no-highlight">12</i>&nbsp;
<i class="no-highlight">13</i>&nbsp;import com.snaplogic.api.ConfigurationException;
<i class="no-highlight">14</i>&nbsp;import com.snaplogic.snap.api.Document;
<i class="no-highlight">15</i>&nbsp;import com.snaplogic.snap.api.DocumentUtility;
<i class="no-highlight">16</i>&nbsp;import com.snaplogic.snap.api.ErrorViews;
<i class="no-highlight">17</i>&nbsp;import com.snaplogic.snap.api.OutputViews;
<i class="no-highlight">18</i>&nbsp;import com.snaplogic.snap.api.SnapDataException;
<i class="no-highlight">19</i>&nbsp;import com.snaplogic.snap.api.sql.DatabaseAccount;
<i class="no-highlight">20</i>&nbsp;import com.snaplogic.snap.api.sql.DatabaseConfig;
<i class="no-highlight">21</i>&nbsp;import com.snaplogic.snap.api.sql.DatabaseUtils;
<i class="no-highlight">22</i>&nbsp;import com.snaplogic.snap.api.sql.DefaultTypeMappingHandler;
<i class="no-highlight">23</i>&nbsp;import com.snaplogic.snap.api.sql.MetadataProvider;
<i class="no-highlight">24</i>&nbsp;import com.snaplogic.snap.api.sql.TableMetaData;
<i class="no-highlight">25</i>&nbsp;import com.snaplogic.snap.api.sql.converters.CustomTypeConverter;
<i class="no-highlight">26</i>&nbsp;import com.snaplogic.snap.api.sql.converters.registry.HiveConverterRegistry;
<i class="no-highlight">27</i>&nbsp;import com.snaplogic.snap.api.sql.mappers.HiveRecordMapper;
<i class="no-highlight">28</i>&nbsp;import com.snaplogic.snap.api.sql.quotation.QuotationHandler;
<i class="no-highlight">29</i>&nbsp;
<i class="no-highlight">30</i>&nbsp;import org.apache.commons.collections.MapUtils;
<i class="no-highlight">31</i>&nbsp;import org.apache.commons.lang3.StringUtils;
<i class="no-highlight">32</i>&nbsp;import org.jooq.DSLContext;
<i class="no-highlight">33</i>&nbsp;import org.jooq.Field;
<i class="no-highlight">34</i>&nbsp;import org.jooq.Query;
<i class="no-highlight">35</i>&nbsp;import org.jooq.SQLDialect;
<i class="no-highlight">36</i>&nbsp;import org.jooq.conf.Settings;
<i class="no-highlight">37</i>&nbsp;import org.jooq.impl.DSL;
<i class="no-highlight">38</i>&nbsp;import org.slf4j.Logger;
<i class="no-highlight">39</i>&nbsp;import org.slf4j.LoggerFactory;
<i class="no-highlight">40</i>&nbsp;
<i class="no-highlight">41</i>&nbsp;import java.sql.BatchUpdateException;
<i class="no-highlight">42</i>&nbsp;import java.sql.SQLException;
<i class="no-highlight">43</i>&nbsp;import java.util.List;
<i class="no-highlight">44</i>&nbsp;import java.util.Map;
<i class="no-highlight">45</i>&nbsp;
<i class="no-highlight">46</i>&nbsp;import javax.inject.Inject;
<i class="no-highlight">47</i>&nbsp;import javax.inject.Named;
<i class="no-highlight">48</i>&nbsp;
<i class="no-highlight">49</i>&nbsp;import static com.snaplogic.snap.api.sql.operations.Messages.RESOLUTION_ADDRESS_ISSUE;
<i class="no-highlight">50</i>&nbsp;import static com.snaplogic.snap.api.sql.operations.Messages.RETRIEVE_DATABASE_METADATA_FAILED;
<i class="no-highlight">51</i>&nbsp;import static com.snaplogic.snap.api.sql.operations.Messages.UNSUPPORTED_OPERATION;
<i class="no-highlight">52</i>&nbsp;
<i class="no-highlight">53</i>&nbsp;/**
<i class="no-highlight">54</i>&nbsp; * Jdbc operations implementations for SparkSQL
<i class="no-highlight">55</i>&nbsp; */
<i class="no-highlight">56</i>&nbsp;public class SparkSQLOperations extends JdbcOperationsImpl {
<b class="nc"><i class="no-highlight">57</i>&nbsp;    private static final Logger LOG = LoggerFactory.getLogger(SparkSQLOperations.class);</b>
<i class="no-highlight">58</i>&nbsp;    private static final String PAREN_REGEX = &quot;\\(.*?\\) &quot;;
<i class="no-highlight">59</i>&nbsp;    private static final int SPARK_SQL_2X = 2;
<i class="no-highlight">60</i>&nbsp;    private Integer majorVersion;
<i class="no-highlight">61</i>&nbsp;
<i class="no-highlight">62</i>&nbsp;    @Inject
<i class="no-highlight">63</i>&nbsp;    protected SparkSQLOperations(HiveRecordMapper recordMapper,
<i class="no-highlight">64</i>&nbsp;            HiveConverterRegistry converterRegistry, final DatabaseUtils databaseUtils,
<i class="no-highlight">65</i>&nbsp;            @Named(SPARKSQL) final MetadataProvider metadataProvider,
<i class="no-highlight">66</i>&nbsp;            DefaultTypeMappingHandler defaultTypeMappingHandler,
<i class="no-highlight">67</i>&nbsp;            @Named(SPARKSQL) CustomTypeConverter customTypeConverter,
<i class="no-highlight">68</i>&nbsp;            @Named(SPARKSQL) final QuotationHandler quotationHandler) {
<b class="nc"><i class="no-highlight">69</i>&nbsp;        super(recordMapper, converterRegistry, databaseUtils, metadataProvider,</b>
<i class="no-highlight">70</i>&nbsp;                defaultTypeMappingHandler, customTypeConverter, quotationHandler);
<b class="nc"><i class="no-highlight">71</i>&nbsp;        runtimeDBName = SPARKSQL;</b>
<i class="no-highlight">72</i>&nbsp;    }
<i class="no-highlight">73</i>&nbsp;
<i class="no-highlight">74</i>&nbsp;    @Override
<i class="no-highlight">75</i>&nbsp;    public int[] insert(final String tableName, final Document document,
<i class="no-highlight">76</i>&nbsp;            final DatabaseAccount account, final TableMetaData tableMetaData,
<i class="no-highlight">77</i>&nbsp;            final DuplicateKeyAction duplicateKeyAction, final boolean preserveCaseSensitiveness,
<i class="no-highlight">78</i>&nbsp;            final OutputViews outputViews, final ErrorViews errorViews,
<i class="no-highlight">79</i>&nbsp;            final DocumentUtility documentUtility, final List&lt;Document&gt; inputDocuments,
<i class="no-highlight">80</i>&nbsp;            final int maxRetries, final long retryInterval) throws BatchUpdateException {
<i class="no-highlight">81</i>&nbsp;        try {
<i class="no-highlight">82</i>&nbsp;            // if Spark SQL version is older than 3.x, insert one record at a time
<b class="nc"><i class="no-highlight">83</i>&nbsp;            if (majorVersion == null) {</b>
<b class="nc"><i class="no-highlight">84</i>&nbsp;                majorVersion = conn.getMetaData().getDatabaseMajorVersion();</b>
<i class="no-highlight">85</i>&nbsp;            }
<b class="nc"><i class="no-highlight">86</i>&nbsp;            if (majorVersion &gt; SPARK_SQL_2X) {</b>
<i class="no-highlight">87</i>&nbsp;                // if Spark SQL version is 3.x or newer, insert in batch processing
<b class="nc"><i class="no-highlight">88</i>&nbsp;                return super.insert(tableName, document, account, tableMetaData, duplicateKeyAction,</b>
<i class="no-highlight">89</i>&nbsp;                        preserveCaseSensitiveness, outputViews, errorViews, documentUtility,
<i class="no-highlight">90</i>&nbsp;                        inputDocuments, maxRetries, retryInterval);
<i class="no-highlight">91</i>&nbsp;            }
<b class="nc"><i class="no-highlight">92</i>&nbsp;        } catch (SQLException e) {</b>
<b class="nc"><i class="no-highlight">93</i>&nbsp;            errorViews.write((SnapDataException)</b>
<i class="no-highlight">94</i>&nbsp;                    new SnapDataException(RETRIEVE_DATABASE_METADATA_FAILED)
<b class="nc"><i class="no-highlight">95</i>&nbsp;                    .withReason(e.getMessage())</b>
<b class="nc"><i class="no-highlight">96</i>&nbsp;                    .withResolution(RESOLUTION_ADDRESS_ISSUE), document);</b>
<b class="nc"><i class="no-highlight">97</i>&nbsp;        }</b>
<b class="nc"><i class="no-highlight">98</i>&nbsp;        if (document != null) {</b>
<b class="nc"><i class="no-highlight">99</i>&nbsp;            final DatabaseConfig databaseConfig = account.getDatabaseConfig();</b>
<b class="nc"><i class="no-highlight">100</i>&nbsp;            final SQLDialect dialect = databaseConfig.getSQLDialect();</b>
<b class="nc"><i class="no-highlight">101</i>&nbsp;            final Settings settings = databaseConfig.getSettings();</b>
<b class="nc"><i class="no-highlight">102</i>&nbsp;            final Map&lt;String, Object&gt; data = document.get(Map.class);</b>
<b class="nc"><i class="no-highlight">103</i>&nbsp;            if (MapUtils.isEmpty(data)) {</b>
<b class="nc"><i class="no-highlight">104</i>&nbsp;                return null;</b>
<i class="no-highlight">105</i>&nbsp;            }
<i class="no-highlight">106</i>&nbsp;
<b class="nc"><i class="no-highlight">107</i>&nbsp;            acquireConnection(account);</b>
<b class="nc"><i class="no-highlight">108</i>&nbsp;            final Map&lt;Field&lt;?&gt;, Object&gt; fieldMap = convertValues(data, tableMetaData, dialect,</b>
<i class="no-highlight">109</i>&nbsp;                    preserveCaseSensitiveness);
<i class="no-highlight">110</i>&nbsp;
<b class="nc"><i class="no-highlight">111</i>&nbsp;            final DSLContext context = DSL.using(conn, dialect, settings);</b>
<b class="nc"><i class="no-highlight">112</i>&nbsp;            final Query query = getInsertQuery(tableName, duplicateKeyAction, fieldMap, context);</b>
<i class="no-highlight">113</i>&nbsp;
<i class="no-highlight">114</i>&nbsp;            try {
<i class="no-highlight">115</i>&nbsp;                // Special handle for SparkSQL, remove the parenthesis for columns
<b class="nc"><i class="no-highlight">116</i>&nbsp;                final String sql = query.getSQL().replaceFirst(PAREN_REGEX, StringUtils.EMPTY);</b>
<b class="nc"><i class="no-highlight">117</i>&nbsp;                LOG.debug(&quot;Executing SparkSQL statement: {}&quot;, sql);</b>
<b class="nc"><i class="no-highlight">118</i>&nbsp;                return executeQueryDirectly(context, query, sql, maxRetries, retryInterval);</b>
<b class="nc"><i class="no-highlight">119</i>&nbsp;            } catch (Throwable e) {</b>
<b class="nc"><i class="no-highlight">120</i>&nbsp;                DatabaseUtils.writeErrorView(errorViews, inputDocuments, e);</b>
<b class="nc"><i class="no-highlight">121</i>&nbsp;                return null;</b>
<i class="no-highlight">122</i>&nbsp;            }
<i class="no-highlight">123</i>&nbsp;        }
<b class="nc"><i class="no-highlight">124</i>&nbsp;        return null;</b>
<i class="no-highlight">125</i>&nbsp;    }
<i class="no-highlight">126</i>&nbsp;
<i class="no-highlight">127</i>&nbsp;    @Override
<i class="no-highlight">128</i>&nbsp;    public int[] merge(final String tableName, final String idColumn, final String onCondition,
<i class="no-highlight">129</i>&nbsp;            final Map&lt;String, Object&gt; onConditionValueMap, final Document document,
<i class="no-highlight">130</i>&nbsp;            final DatabaseAccount account, final TableMetaData tableMetaData,
<i class="no-highlight">131</i>&nbsp;            final OutputViews outputViews, final ErrorViews errorViews,
<i class="no-highlight">132</i>&nbsp;            final DocumentUtility documentUtility, final List&lt;Document&gt; inputDocuments,
<i class="no-highlight">133</i>&nbsp;            final int maxRetries, final long retryInterval) throws BatchUpdateException {
<i class="no-highlight">134</i>&nbsp;        // Return an exception here since we don&#39;t have a Merge Snap for Delta Lake or Generic JDBC
<b class="nc"><i class="no-highlight">135</i>&nbsp;        throw new ConfigurationException(UNSUPPORTED_OPERATION);</b>
<i class="no-highlight">136</i>&nbsp;    }
<i class="no-highlight">137</i>&nbsp;
<i class="no-highlight">138</i>&nbsp;    @Override
<i class="no-highlight">139</i>&nbsp;    public int[] executeStatement(final String statement, final List&lt;Object&gt; bindValues,
<i class="no-highlight">140</i>&nbsp;            final DatabaseAccount account, final boolean isQueryWithoutBindingParameter,
<i class="no-highlight">141</i>&nbsp;            final int maxRetries, final long retryInterval) throws BatchUpdateException {
<b class="nc"><i class="no-highlight">142</i>&nbsp;        final DatabaseConfig databaseConfig = account.getDatabaseConfig();</b>
<b class="nc"><i class="no-highlight">143</i>&nbsp;        final SQLDialect dialect = databaseConfig.getSQLDialect();</b>
<b class="nc"><i class="no-highlight">144</i>&nbsp;        final Settings settings = databaseConfig.getSettings();</b>
<b class="nc"><i class="no-highlight">145</i>&nbsp;        acquireConnection(account);</b>
<b class="nc"><i class="no-highlight">146</i>&nbsp;        final DSLContext context = DSL.using(conn, dialect, settings);</b>
<b class="nc"><i class="no-highlight">147</i>&nbsp;        Query query = context.query(statement, bindValues.toArray());</b>
<i class="no-highlight">148</i>&nbsp;        // Special handle for SparkSQL, SparkSQL doesn&#39;t support batch, so we may need to run
<i class="no-highlight">149</i>&nbsp;        // them directly
<b class="nc"><i class="no-highlight">150</i>&nbsp;        final String sql = query.getSQL();</b>
<b class="nc"><i class="no-highlight">151</i>&nbsp;        LOG.debug(&quot;Executing SparkSQL statement: {}&quot;, sql);</b>
<b class="nc"><i class="no-highlight">152</i>&nbsp;        return executeQueryDirectly(context, query, sql, maxRetries, retryInterval);</b>
<i class="no-highlight">153</i>&nbsp;    }
<i class="no-highlight">154</i>&nbsp;
<i class="no-highlight">155</i>&nbsp;    @Override
<i class="no-highlight">156</i>&nbsp;    public String getName() {
<b class="nc"><i class="no-highlight">157</i>&nbsp;        return SPARKSQL;</b>
<i class="no-highlight">158</i>&nbsp;    }
<i class="no-highlight">159</i>&nbsp;}
</div>
</pre>
</div>

<script type="text/javascript">
(function() {
    var msie = false, msie9 = false;
    /*@cc_on
      msie = true;
      @if (@_jscript_version >= 9)
        msie9 = true;
      @end
    @*/

    if (!msie || msie && msie9) {
        var codeBlock = document.getElementById('sourceCode');

        if (codeBlock) {
            hljs.highlightBlock(codeBlock);
        }
    }
})();
</script>

<div class="footer">
    
    <div style="float:right;">generated on 2024-01-19 15:49</div>
</div>
</body>
</html>
